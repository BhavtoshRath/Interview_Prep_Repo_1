{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Understanding & Modifying Existing TensorFlow Code**  \n",
    "*These questions test how well you can read, modify, and improve existing ML models.*\n",
    "\n",
    "**Q1: Identify issues in the given TensorFlow model**  \n",
    "_You are given a simple MLP model for binary classification in TensorFlow. Can you spot and fix any issues?_ \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- **How would you fix the loss function issue?**   \n",
    "'mse' is used for linear regression tasks. 'Sigmoid' is usually used for binary classification, so correct loss function is binary_crossentropy\n",
    "- **What happens if we donâ€™t specify an `input_shape` in the first layer?**  \n",
    "`Deferred execution` or `lazy building`. The model will dynamically infer the input shape from the data provided during training or inference.\n",
    "\n",
    "\n",
    "### **2. Debugging ML Code Using Documentation & Error Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: Debugging a Training Pipeline**\n",
    "\n",
    "**Error message:**\n",
    "```\n",
    "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).\n",
    "```\n",
    "\n",
    "#### **Follow-up:**  \n",
    "\n",
    "**What is causing this error?**  \n",
    "This error occurs when the data types of the input data (`X_train` or `y_train`) are not supported by TensorFlow. Specifically, TensorFlow expects the data to be of types like `float32`, `float64`, `int32`, or `int64`. The error suggests that the data contains an unsupported type, such as a Python `int` or another non-Numeric type that NumPy or TensorFlow cannot handle.\n",
    "\n",
    "#### **How would you fix it?**  \n",
    "To fix this error:\n",
    "1. **Ensure that `X_train` and `y_train` are NumPy arrays or TensorFlow tensors.**  \n",
    "   You can convert the data to NumPy arrays or TensorFlow tensors with the correct type.\n",
    "\n",
    "2. **Check and convert the data types to supported formats:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays with the correct dtype if necessary\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.int64)  # For classification tasks\n",
    "```\n",
    "\n",
    "3. **Ensure that the labels (`y_train`) are integers** for classification tasks, or **floats for regression tasks.**\n",
    "\n",
    "4. **Verify that no other data structures are passed** (such as lists or Python `int`) that are incompatible with TensorFlow.\n",
    "\n",
    "#### **How does TensorFlow handle data type conversions?**  \n",
    "TensorFlow automatically converts certain input types (like Python lists) into tensors with the appropriate data types. However, if it encounters a data type that doesn't fit within its expected range (e.g., unsupported objects), it throws a `ValueError`. TensorFlow can handle types like:\n",
    "- NumPy arrays (`np.float32`, `np.int64`, etc.)\n",
    "- Tensors (`tf.float32`, `tf.int64`, etc.)\n",
    "\n",
    "TensorFlow will try to cast these to the proper type when required, but explicit type conversion (as shown above) is usually a good practice to prevent issues.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Evaluating ML Model Performance**\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: How would you evaluate a ranking model for Etsy Ads?**\n",
    "\n",
    "- **What metrics would you use?**  \n",
    "  You would typically use **ranking-specific metrics** such as:\n",
    "  - **NDCG (Normalized Discounted Cumulative Gain):** Measures the quality of the ranking by considering the position of relevant items.\n",
    "  - **Precision@K (P@K):** Evaluates how many relevant items appear in the top `K` results.\n",
    "  - **Click-Through Rate (CTR):** Measures the percentage of impressions that resulted in clicks.\n",
    "  - **Mean Reciprocal Rank (MRR):** Evaluates the position of the first relevant item.\n",
    "\n",
    "- **How would you handle class imbalance in evaluation?**  \n",
    "  You could handle class imbalance in the following ways:\n",
    "  - **Weighted metrics:** Use weighted versions of Precision, Recall, or NDCG that give more importance to underrepresented classes.\n",
    "  - **Synthetic data generation:** Augment the data using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "  - **Resampling:** Use oversampling or undersampling techniques to balance the dataset.\n",
    "\n",
    "- **How do you know if a model is overfitting?**  \n",
    "  You can detect overfitting by:\n",
    "  - Monitoring the **training loss** and **validation loss** over epochs. If the training loss keeps decreasing while the validation loss increases, overfitting is likely.\n",
    "  - Checking the **validation accuracy**: If it remains stagnant or decreases while training accuracy increases, overfitting is happening.\n",
    "  - Use **cross-validation** to ensure consistent model performance.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4: You trained a TensorFlow model, and its accuracy is high, but the business metric (e.g., conversions) is low. What could be wrong?**\n",
    "\n",
    "- **How would you debug this issue?**  \n",
    "  Possible causes include:\n",
    "  1. **Mismatch between training accuracy and business goal:** The model might optimize for accuracy but not align with business metrics like conversions. You may need to focus on **precision, recall, or other business-relevant metrics**.\n",
    "  2. **Data leakage or misalignment between training and test data**: Verify that the data is preprocessed correctly and that there is no leakage between the training and validation sets.\n",
    "  3. **Class imbalance or skewed distribution**: If conversions are rare, the model might focus on predicting the majority class.\n",
    "  \n",
    "  **Solution:**  \n",
    "  - Incorporate business metrics (e.g., **conversions**) directly into the loss function or as evaluation metrics.\n",
    "  - Use **ROC-AUC, F1 score**, or **precision/recall curves** to better align model predictions with business objectives.\n",
    "\n",
    "- **What additional evaluation strategies would you use?**  \n",
    "  - **Monitor business KPIs directly**: Track the business impact (e.g., conversion rate, sales) as a key evaluation metric.\n",
    "  - Use **ablation studies** to analyze how different model components impact conversions.\n",
    "  - **Post-deployment analysis**: If possible, A/B test the model in production to directly observe business outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Scalability & Performance Considerations**\n",
    "\n",
    "---\n",
    "\n",
    "**Q5: Your TensorFlow model is slow during inference. How do you optimize it?**\n",
    "\n",
    "- **How would you reduce latency in model serving?**\n",
    "  - Use **TensorFlow Lite** for mobile or embedded devices.\n",
    "  - Implement **model quantization** (e.g., `float32` to `int8`) to reduce the model size and speed up inference.\n",
    "  - **Batch inference**: Process multiple inputs in one pass to optimize throughput.\n",
    "\n",
    "- **What techniques can be used to optimize TensorFlow models for speed?**\n",
    "  - **TensorRT**: Optimize the model for NVIDIA GPUs.\n",
    "  - **Model pruning**: Remove unnecessary weights or neurons to reduce computation.\n",
    "  - **Graph optimization**: Use `tf.function` for graph execution to enable optimizations like XLA (Accelerated Linear Algebra).\n",
    "  - **Distributed inference**: Use multiple machines to handle high request volumes.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6: How would you scale a recommendation system for millions of Etsy users and items?**\n",
    "\n",
    "- **Would you precompute embeddings or compute them in real-time?**\n",
    "  - For large-scale systems, **precomputing embeddings** for users and items is usually the better approach to reduce real-time latency. Store embeddings in a distributed database like Redis or Elasticsearch for quick retrieval.\n",
    "  \n",
    "- **How would you store and retrieve features efficiently?**\n",
    "  - Use a **feature store** to store and retrieve features efficiently. For example:\n",
    "    - **HDFS or cloud-based storage**: Store features in a distributed file system for easy access across multiple machines.\n",
    "    - **NoSQL databases** (like MongoDB, Cassandra): These are good for fast lookups of user/item features in a recommendation system.\n",
    "    - Use **caching** (e.g., Redis) to store frequently accessed features for real-time serving.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Handling Edge Cases & Model Robustness**\n",
    "\n",
    "---\n",
    "\n",
    "**Q7: Your ad ranking model gives high scores to irrelevant ads. What could be going wrong?**\n",
    "\n",
    "- **How would you debug feature importance?**\n",
    "  - Use **SHAP (SHapley Additive exPlanations)** or **LIME** to explain model predictions and understand which features are causing the model to give high scores to irrelevant ads.\n",
    "  - Analyze the feature distribution for the irrelevant ads and check if there's any bias in the data.\n",
    "\n",
    "- **Would you adjust the loss function or introduce new features?**\n",
    "  - You could introduce new features such as:\n",
    "    - **User behavioral data** (click history, session time).\n",
    "    - **Item attributes** (e.g., category, price).\n",
    "  - If the model doesn't distinguish between relevant and irrelevant ads, consider modifying the **loss function** to penalize irrelevant ads more heavily (e.g., using weighted cross-entropy).\n",
    "\n",
    "---\n",
    "\n",
    "**Q8: Your ML model performs well on training data but poorly on new listings (cold start problem). How do you handle this?**\n",
    "\n",
    "- **What strategies would you use to rank new Etsy listings fairly?**\n",
    "  - Use **content-based filtering** where you rank new listings based on their attributes (e.g., category, price, description).\n",
    "  - Incorporate **metadata features** for new items and leverage **heuristics** such as ranking based on similar past listings.\n",
    "\n",
    "- **Would you use zero-shot learning, transfer learning, or heuristics?**\n",
    "  - **Zero-shot learning** can be effective in certain scenarios where you can use semantic embeddings (e.g., from a pre-trained transformer model) to rank new items without needing labeled training data.\n",
    "  - **Transfer learning** using a pre-trained model can be fine-tuned on Etsy-specific data to handle new listings.\n",
    "  - **Heuristics** based on item metadata could provide an initial ranking until more data is available for training.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Working with TensorFlow and ML Libraries**\n",
    "\n",
    "---\n",
    "\n",
    "**Q9: What are the differences between `tf.data.Dataset` and using NumPy arrays for feeding data into TensorFlow models?**\n",
    "\n",
    "- **When would you use `tf.data.Dataset`?**\n",
    "  - Use `tf.data.Dataset` when you need efficient data loading, processing, and pipeline integration (e.g., batching, shuffling, prefetching).\n",
    "  - It is especially useful for large datasets that cannot fit in memory or require transformations during training.\n",
    "  \n",
    "- **How can you optimize dataset loading for large-scale training?**\n",
    "  - Use `tf.data` features like **batching**, **prefetching**, **shuffling**, and **parallel mapping** to optimize dataset loading for large-scale training.\n",
    "\n",
    "```python\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Q10: Your training loss is NaN in TensorFlow. How do you debug it?**\n",
    "\n",
    "- **What could be the possible causes?**\n",
    "  - **Exploding gradients**: The gradients may become too large, causing the loss to become NaN.\n",
    "  - **Incorrect data preprocessing**: Check if there are NaNs or infinities in the dataset.\n",
    "  - **Incorrect loss function**: For example, using a regression loss function for a classification task.\n",
    "  \n",
    "- **How would you use TensorFlow debugging tools (`tf.debugging`) to investigate?**\n",
    "  - Use `tf.debugging.check_numerics` to catch NaN or Inf values in the tensors.\n",
    "\n",
    "```python\n",
    "tf.debugging.check_numerics(loss, 'NaN or Inf detected')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like any clarification or further exploration of these solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
