{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are **5 more TensorFlow debugging and error analysis questions** that cover newer or less common errors you might encounter in modern TensorFlow workflows. These examples will help you practice identifying and resolving issues in more advanced or specific scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 1: Debugging Mixed Precision Training**\n",
    "You are using mixed precision training (`tf.keras.mixed_precision`), but you encounter the following error:\n",
    "```\n",
    "TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\n",
    "```\n",
    "\n",
    "#### **Code**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Enable mixed precision\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((1000, 32), dtype=tf.float32)\n",
    "y_train = tf.random.uniform((1000,), maxval=2, dtype=tf.int32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "```\n",
    "\n",
    "#### **Issue**:\n",
    "The error occurs because `y_train` is of type `int32`, but mixed precision training requires all inputs to be compatible with `float16`.\n",
    "\n",
    "#### **Fix**:\n",
    "Convert `y_train` to `float32` (or `float16` if supported).\n",
    "\n",
    "```python\n",
    "# Convert y_train to float32\n",
    "y_train = tf.cast(y_train, dtype=tf.float32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 2: Debugging a Custom Training Loop with Gradient Tape**\n",
    "You are using a custom training loop with `GradientTape`, but you encounter the following error:\n",
    "```\n",
    "AttributeError: 'NoneType' object has no attribute 'numpy'\n",
    "```\n",
    "\n",
    "#### **Code**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((1000, 32))\n",
    "y_train = tf.random.normal((1000, 1))\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(10):\n",
    "    for i in range(0, len(X_train), 32):\n",
    "        X_batch = X_train[i:i+32]\n",
    "        y_batch = y_train[i:i+32]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_batch, training=True)\n",
    "            loss = loss_fn(y_batch, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    print(f\"Loss: {loss.numpy()}\")  # Error occurs here\n",
    "```\n",
    "\n",
    "#### **Issue**:\n",
    "The error occurs because `loss` is not defined outside the `GradientTape` scope for the last batch.\n",
    "\n",
    "#### **Fix**:\n",
    "Ensure `loss` is defined for the last batch by moving the print statement inside the loop.\n",
    "\n",
    "```python\n",
    "# Print loss inside the loop\n",
    "for epoch in range(10):\n",
    "    for i in range(0, len(X_train), 32):\n",
    "        X_batch = X_train[i:i+32]\n",
    "        y_batch = y_train[i:i+32]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_batch, training=True)\n",
    "            loss = loss_fn(y_batch, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        print(f\"Loss: {loss.numpy()}\")  # Print inside the loop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ Debugging TensorFlow Gradient Issues**  \n",
    "üìå *Objective: Fix issues when training a model using `tf.GradientTape`.*  \n",
    "\n",
    "### **‚ùì Question:**  \n",
    "This code runs without errors but **doesn't update model weights**. What‚Äôs wrong?  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Simple model\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "X = tf.random.normal((10, 5))\n",
    "y = tf.random.normal((10, 1))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **‚úÖ Solution:**  \n",
    "- `tape.gradient(loss, model.trainable_variables)` returns **None** if **no gradients were recorded**.  \n",
    "- The issue: **AutoGraph optimizations strip away gradient tracking** when `tf.function` is used. \n",
    "\n",
    "#### **‚úÖ Fixed Code:**\n",
    "```python\n",
    "with tf.GradientTape(persistent=True) as tape:  # ‚úÖ Use persistent tape\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "```\n",
    "Now, gradients will be correctly computed.\n",
    "\n",
    "\n",
    "#### Possible `GradientTape` related issues:  \n",
    "  1. **`tf.GradientTape` is not persistent**  \n",
    "     - If you reuse `tape.gradient()`, set `persistent=True`:  \n",
    "       ```python\n",
    "       with tf.GradientTape(persistent=True) as tape:\n",
    "       ```\n",
    "  2. **Tensors are not being watched**  \n",
    "     - If you manually create tensors, ensure they are watched:  \n",
    "       ```python\n",
    "       tape.watch(X_train)\n",
    "       ```\n",
    "  3. **Loss function is not differentiable**  \n",
    "     - Ensure `y_train` has the correct shape (use `float32` for continuous loss functions).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Question 3: Debugging a Distributed Training Issue**\n",
    "You are using `tf.distribute.MirroredStrategy` for distributed training, but you encounter the following error:\n",
    "```\n",
    "ValueError: Detected dataset sharding policy `OFF` which is not allowed for distributed training.\n",
    "```\n",
    "\n",
    "#### **Code**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a distributed strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Create a dataset\n",
    "X_train = tf.random.normal((1000, 32))\n",
    "y_train = tf.random.uniform((1000,), maxval=2, dtype=tf.int32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "# Define and compile the model within the strategy scope\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset, epochs=10)\n",
    "```\n",
    "\n",
    "#### **Issue**:\n",
    "The error occurs because the dataset is not sharded properly for distributed training.\n",
    "\n",
    "#### **Fix**:\n",
    "Use `strategy.experimental_distribute_dataset` to shard the dataset.\n",
    "\n",
    "```python\n",
    "# Shard the dataset for distributed training\n",
    "distributed_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "# Train the model\n",
    "model.fit(distributed_dataset, epochs=10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 4: Debugging a SavedModel Loading Issue**\n",
    "You saved a model using `tf.saved_model.save`, but you encounter the following error when loading it:\n",
    "```\n",
    "KeyError: 'layer_name not found in checkpoint'\n",
    "```\n",
    "\n",
    "#### **Code**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define and save the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.save('my_model')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.saved_model.load('my_model')\n",
    "```\n",
    "\n",
    "#### **Issue**:\n",
    "The error occurs because `tf.saved_model.load` is used instead of `tf.keras.models.load_model`.\n",
    "\n",
    "#### **Fix**:\n",
    "Use `tf.keras.models.load_model` to load a Keras model.\n",
    "\n",
    "```python\n",
    "# Load the model correctly\n",
    "loaded_model = tf.keras.models.load_model('my_model')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 5: Debugging a TensorFlow Lite Conversion Issue**\n",
    "You are converting a TensorFlow model to TensorFlow Lite, but you encounter the following error:\n",
    "```\n",
    "ConverterError: None is only supported in the 1st dimension. Tensor 'input' has invalid shape '[None, None]'.\n",
    "```\n",
    "\n",
    "#### **Code**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 32)),  # Variable-length input (BR)\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Convert to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "```\n",
    "\n",
    "#### **Issue**:\n",
    "The error occurs because TensorFlow Lite does not support variable-length dimensions except for the first dimension.\n",
    "\n",
    "#### **Fix**:\n",
    "Specify a fixed input shape for the model.\n",
    "\n",
    "```python\n",
    "# Define the model with a fixed input shape\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(100, 32)),  # Fixed input shape\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Convert to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "These examples cover newer or less common errors in TensorFlow, including:\n",
    "1. Mixed precision training.\n",
    "2. Custom training loops.\n",
    "3. Distributed training.\n",
    "4. SavedModel loading.\n",
    "5. TensorFlow Lite conversion.\n",
    "\n",
    "Practicing these will help you handle advanced TensorFlow scenarios and debug complex issues effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are **five more TensorFlow debugging and error analysis questions** covering additional issues that might arise in training deep learning models.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Debugging TensorFlow Dataset Pipeline Errors**  \n",
    "**Question:**  \n",
    "The following code attempts to create a `tf.data.Dataset` pipeline but throws an error. Fix it.  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((100, 10))\n",
    "y_train = tf.random.uniform((100,), maxval=2, dtype=tf.int32)\n",
    "\n",
    "# Create dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(100).batch(32)\n",
    "\n",
    "# Iterate through dataset\n",
    "for X_batch, y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "```\n",
    "\n",
    "**Error:**  \n",
    "```\n",
    "ValueError: `batch()` requires elements to have the same shape.\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "Ensure that `y_train` has the correct shape. The issue occurs when `y_train` is a **scalar (shape=())** rather than a **vector (shape=(1,))** per sample.  \n",
    "\n",
    "```python\n",
    "y_train = tf.expand_dims(y_train, axis=-1)  # Fix the shape of labels\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Fixing Model Freezing During Training (No Improvement in Loss)**  \n",
    "**Question:**  \n",
    "You notice that your model‚Äôs loss **stays the same** across epochs. What could be wrong?  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((1000, 10))\n",
    "y_train = tf.random.uniform((1000,), maxval=3, dtype=tf.int32)  # 3 classes\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    Dense(10, activation='relu', input_shape=(10,)),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with incorrect learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),  \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "- **The learning rate is too low (`1e-6`)**, making weight updates insignificant. Increase it to `1e-3` or `1e-4`.  \n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  \n",
    "```\n",
    "\n",
    "- If the issue persists, check for **vanishing gradients** and add **batch normalization** or **ReLU activation**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Debugging Mixed Data Type Errors in TensorFlow**  \n",
    "**Question:**  \n",
    "The following code produces a **dtype error**. Identify and fix the issue.  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create dataset\n",
    "X_train = tf.random.normal((100, 10))\n",
    "y_train = tf.random.uniform((100,), maxval=2, dtype=tf.float32)  # Incorrect dtype for classification\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with incompatible dtype\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "**Error:**  \n",
    "```\n",
    "InvalidArgumentError: Expected binary labels (0 or 1), but found floating point values.\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "The labels (`y_train`) should be **integer type (`tf.int32`)** instead of `float32`. Convert the labels:  \n",
    "\n",
    "```python\n",
    "y_train = tf.cast(y_train, tf.int32)  # Fix the dtype issue\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Fixing Model Output Shape Mismatch**  \n",
    "**Question:**  \n",
    "You get an error when compiling the model. What‚Äôs wrong with the following code?  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((100, 10))\n",
    "y_train = tf.random.uniform((100, 3), maxval=2, dtype=tf.int32)  # Multi-label problem\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    Dense(10, activation='relu', input_shape=(10,)),\n",
    "    Dense(1, activation='sigmoid')  # Single output for multi-label problem\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "**Error:**  \n",
    "```\n",
    "ValueError: logits and labels must have the same shape.\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "Since `y_train` has **three labels per sample**, the final layer should have **three output neurons instead of one**.  \n",
    "\n",
    "```python\n",
    "Dense(3, activation='sigmoid')  # Match output shape to label shape\n",
    "```\n",
    "**For multi-label classifcation, o/p neurons should not be 1**\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Handling Memory Issues with Large Datasets**  \n",
    "**Question:**  \n",
    "When training on a large dataset, your **GPU runs out of memory**. How do you fix it?  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1024)  # Large batch size causing memory error\n",
    "```\n",
    "\n",
    "**Error:**  \n",
    "```\n",
    "ResourceExhaustedError: Out of memory when allocating tensor.\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "- Reduce batch size to **fit within available GPU memory**:  \n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128)\n",
    "```\n",
    "\n",
    "- Enable **mixed precision training** to reduce memory usage:  \n",
    "\n",
    "```python\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "```\n",
    "\n",
    "- Use **data pipeline optimizations** (e.g., `tf.data.Dataset` with `prefetch()` to avoid memory bottlenecks).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of New Debugging Issues and Fixes**\n",
    "| Issue | Fix |\n",
    "|---------------------------|----------------------------------------|\n",
    "| `tf.data.Dataset` shape mismatch | Ensure `y_train` has the correct shape |\n",
    "| Model loss does not change | Increase learning rate and check vanishing gradients |\n",
    "| Mixed dtype error | Convert labels to correct dtype (`int32` for classification) |\n",
    "| Output shape mismatch | Ensure model‚Äôs output neurons match label shape |\n",
    "| GPU memory issues | Reduce batch size, use `mixed_precision`, optimize dataset |\n",
    "\n",
    "---\n",
    "\n",
    "**Question:**  \n",
    "What is wrong with the following TensorFlow snippet?  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([1, 2, 3])\n",
    "y = x + 5\n",
    "print(y.numpy())\n",
    "```\n",
    "\n",
    "**Error Message:**  \n",
    "`AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'numpy'`  \n",
    "\n",
    "**Solution:**  \n",
    "The issue arises when TensorFlow‚Äôs eager execution is disabled (e.g., in graph mode). To fix this, ensure eager execution is enabled:  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.run_functions_eagerly(True)  # Ensure eager execution\n",
    "x = tf.constant([1, 2, 3])\n",
    "y = x + 5\n",
    "print(y.numpy())  # Works correctly\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **2. Debugging: Ranking Model Produces Near-Identical Scores**\n",
    "#### (Similar to NaNs debugging)\n",
    "#### **Problem:**  \n",
    "Your **ad ranking model** assigns nearly **identical scores** to all ads, making ranking ineffective.  \n",
    "\n",
    "#### **Possible Causes & Fixes:**  \n",
    "‚úÖ **Cause 1: Over-Regularization**  \n",
    "- If **L2 regularization is too strong**, model produces similar scores.  \n",
    "- **Fix:** Reduce regularization:  \n",
    "  ```python\n",
    "  keras.regularizers.l2(0.0001)  # Reduce weight decay\n",
    "  ```\n",
    "\n",
    "‚úÖ **Cause 2: Vanishing Gradient in Deep Model**  \n",
    "- If activation functions saturate, gradients shrink to zero.  \n",
    "- **Fix:**  \n",
    "  - Replace `sigmoid` with `swish` or `ReLU`.  \n",
    "  - Use **batch normalization** before activations.\n",
    "\n",
    "‚úÖ **Cause 3: Data Leakage During Training**  \n",
    "- If test data is **leaked** during training, scores might not vary.  \n",
    "- **Fix:** Verify train-test split:  \n",
    "  ```python\n",
    "  assert set(X_train.index).isdisjoint(set(X_test.index))\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like more **TensorFlow debugging scenarios**, or are you interested in **specific types of errors**? üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
