{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interview Questions: Evaluating ML Model Performance**  \n",
    "\n",
    "These questions focus on **model evaluation, metrics, and debugging performance issues** in machine learning, particularly in TensorFlow. Each question includes a code-based solution.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Q1: Compute Accuracy, Precision, Recall, and F1-Score for a Model**  \n",
    "**Task:** After training a classification model, compute **accuracy, precision, recall, and F1-score** using TensorFlow/Keras.  \n",
    "\n",
    "### **Solution:**  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load a dataset (MNIST for example)\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize\n",
    "\n",
    "# Define a simple model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = tf.argmax(y_pred, axis=1).numpy()  # Convert probabilities to class labels\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "precision = precision_score(y_test, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_test, y_pred_classes, average='macro')\n",
    "f1 = f1_score(y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "## **Q2: Plot ROC Curve and Compute AUC Score**  \n",
    "**Task:** Modify the model evaluation process to include the **ROC curve** and **AUC score** for binary classification.  \n",
    "\n",
    "### **Solution:**  \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Assume we have a binary classification model (MNIST: 0 vs non-0)\n",
    "y_test_binary = (y_test == 0).astype(int)  # Convert to binary (0 vs rest)\n",
    "y_pred_prob = y_pred[:, 0]  # Take probability for class 0\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test_binary, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random guess line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "```\n",
    "---\n",
    "\n",
    "## **Q3: Identify and Debug Model Overfitting**  \n",
    "**Task:** Implement **early stopping** and **dropout** to prevent overfitting.  \n",
    "\n",
    "### **Solution:**  \n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define model with dropout\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),  # Dropout added\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "## **Q4: Handle Class Imbalance with Weighted Loss**  \n",
    "**Task:** Modify the model to **handle class imbalance** by using **class weights**.  \n",
    "\n",
    "### **Solution:**  \n",
    "```python\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assume imbalanced classes in y_train\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Train model with class weights\n",
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), class_weight=class_weight_dict)\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "## **Q5: Use K-Fold Cross-Validation for Model Evaluation**  \n",
    "**Task:** Evaluate the model using **5-fold cross-validation** to get a more reliable estimate of performance.  \n",
    "\n",
    "### **Solution:**  \n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Define and train model\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=3, verbose=0)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)[1]\n",
    "    accuracies.append(val_accuracy)\n",
    "\n",
    "print(f\"Mean cross-validation accuracy: {np.mean(accuracies):.4f}\")\n",
    "```\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "‚úÖ Use **precision, recall, and F1-score** for better insights in classification.  \n",
    "‚úÖ **ROC-AUC** helps evaluate binary classifiers beyond accuracy.  \n",
    "‚úÖ **EarlyStopping & Dropout** prevent **overfitting**.  \n",
    "‚úÖ Use **class weights** when handling **imbalanced datasets**.  \n",
    "‚úÖ **K-Fold cross-validation** ensures stable and reliable performance estimates.  \n",
    "\n",
    "Let me know if you need **harder** interview questions! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some **interview questions focused on evaluating ML model performance**, including **code correction**-type problems.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Fix Incorrect Accuracy Calculation**\n",
    "**‚ùì Question:**  \n",
    "You are given a model trained on a dataset. The following code tries to compute the accuracy but produces incorrect results. Can you **identify and correct the mistake**?  \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1])\n",
    "y_pred_probs = np.array([0.8, 0.4, 0.7, 0.6, 0.3, 0.9])\n",
    "\n",
    "accuracy = np.mean(y_pred_probs == y_true)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "### **‚úÖ Solution (Fix the Comparison)**\n",
    "The mistake is comparing **probabilities** (`y_pred_probs`) with **true labels (`y_true`)** directly. We need to convert predictions to binary labels before computing accuracy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1])\n",
    "y_pred_probs = np.array([0.8, 0.4, 0.7, 0.6, 0.3, 0.9])\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5)\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_true)\n",
    "print(\"Accuracy:\", accuracy)  # ‚úÖ Corrected version\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Fix Incorrect AUC Score Calculation**\n",
    "**‚ùì Question:**  \n",
    "The following code is supposed to compute the **AUC-ROC score**, but it raises an error. What‚Äôs wrong, and how would you fix it?\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 1, 0, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_true, y_pred)\n",
    "print(\"AUC-ROC Score:\", auc_score)\n",
    "```\n",
    "\n",
    "### **‚úÖ Solution (Use Probabilities Instead of Binary Predictions)**\n",
    "AUC-ROC requires **probabilistic scores**, not discrete predictions (0 or 1). The fix is to pass probability scores instead.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred_probs = [0.9, 0.2, 0.8, 0.7, 0.1, 0.85]  # ‚úÖ Use probability scores\n",
    "\n",
    "auc_score = roc_auc_score(y_true, y_pred_probs)\n",
    "print(\"AUC-ROC Score:\", auc_score)  # ‚úÖ Corrected version\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Incorrect Precision & Recall Calculation**\n",
    "**‚ùì Question:**  \n",
    "There is a mistake in how precision and recall are calculated in the following code. Find and fix it.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred_probs = [0.8, 0.3, 0.7, 0.6, 0.2, 0.9]\n",
    "\n",
    "precision = precision_score(y_true, y_pred_probs)\n",
    "recall = recall_score(y_true, y_pred_probs)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "```\n",
    "\n",
    "### **‚úÖ Solution (Convert Probabilities to Binary Labels)**\n",
    "Precision and recall expect **binary predictions**, not probabilities. Convert them before passing them into `precision_score` and `recall_score`.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred_probs = [0.8, 0.3, 0.7, 0.6, 0.2, 0.9]\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5)\n",
    "y_pred = (np.array(y_pred_probs) >= 0.5).astype(int)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)  # ‚úÖ Fixed\n",
    "print(\"Recall:\", recall)        # ‚úÖ Fixed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Incorrect F1 Score Calculation**\n",
    "**‚ùì Question:**  \n",
    "The F1 score calculation below is producing incorrect results. What‚Äôs wrong, and how would you correct it?\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred_probs = [0.7, 0.2, 0.6, 0.8, 0.3, 0.9]\n",
    "\n",
    "f1 = f1_score(y_true, y_pred_probs)\n",
    "print(\"F1 Score:\", f1)\n",
    "```\n",
    "\n",
    "### **‚úÖ Solution (Use Binary Predictions Instead of Probabilities)**\n",
    "The issue is that **`f1_score` requires binary labels, not probabilities**. We must first threshold the probability predictions.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred_probs = [0.7, 0.2, 0.6, 0.8, 0.3, 0.9]\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (np.array(y_pred_probs) >= 0.5).astype(int)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", f1)  # ‚úÖ Fixed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ Fix Overfitting Detection with Validation Loss**\n",
    "**‚ùì Question:**  \n",
    "The following code tries to detect overfitting by comparing training and validation loss. However, it doesn‚Äôt correctly detect overfitting. What‚Äôs wrong?\n",
    "\n",
    "```python\n",
    "if train_loss < val_loss:\n",
    "    print(\"Overfitting detected!\")\n",
    "```\n",
    "\n",
    "### **‚úÖ Solution (Fix the Condition)**\n",
    "- Overfitting occurs when **validation loss increases while training loss decreases**.\n",
    "- The correct condition should be checking **whether validation loss is higher than training loss by a significant margin**.\n",
    "\n",
    "```python\n",
    "if val_loss > train_loss + 0.01:  # ‚úÖ Add margin to detect overfitting\n",
    "    print(\"Overfitting detected!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Common Mistakes in Model Evaluation**\n",
    "‚úÖ **Comparing probabilities instead of binary predictions**  \n",
    "‚úÖ **Passing hard labels instead of probability scores for AUC-ROC**  \n",
    "‚úÖ **Confusing `train_loss < val_loss` with actual overfitting detection**  \n",
    "‚úÖ **Incorrect metric usage (e.g., using precision/recall on probabilities)**  \n",
    "\n",
    "Would you like more **advanced debugging questions**? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some **advanced debugging questions** related to **evaluating ML model performance**, including **error analysis, debugging loss functions, and incorrect metric calculations**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Debugging a Model with Perfect Accuracy but Poor Business Metrics**\n",
    "**‚ùì Question:**  \n",
    "You trained a TensorFlow classification model, and it achieves **99% accuracy** on the test set. However, business metrics (e.g., **conversion rate, revenue**) remain **low**. What could be wrong? How would you debug this issue?\n",
    "\n",
    "### **‚úÖ Solution**\n",
    "Possible issues and debugging strategies:\n",
    "1. **Class Imbalance** ‚Äì If one class dominates the dataset, accuracy alone is misleading.  \n",
    "   - ‚úÖ Check class distribution:  \n",
    "     ```python\n",
    "     from collections import Counter\n",
    "     print(Counter(y_test))  # Check class imbalance\n",
    "     ```\n",
    "   - ‚úÖ Use **F1-score**, **Precision-Recall** instead of accuracy:  \n",
    "     ```python\n",
    "     from sklearn.metrics import classification_report\n",
    "     print(classification_report(y_test, model.predict(X_test)))\n",
    "     ```\n",
    "\n",
    "2. **Wrong Business Metric Optimization** ‚Äì If the model **predicts the wrong target**, business impact is low.  \n",
    "   - ‚úÖ Check if the label correlates with the business outcome:  \n",
    "     ```python\n",
    "     import pandas as pd\n",
    "     print(pd.crosstab(y_test, business_metric))  # Verify correlation\n",
    "     ```\n",
    "\n",
    "3. **Poor Generalization to Real Data** ‚Äì If the model overfits, test data may not reflect real-world inputs.  \n",
    "   - ‚úÖ Check train/test distribution shift using KS test:  \n",
    "     ```python\n",
    "     from scipy.stats import ks_2samp\n",
    "     for col in X_train.columns:\n",
    "         print(f\"{col}: {ks_2samp(X_train[col], X_test[col]).pvalue}\")  # Low p-value means a shift\n",
    "     ```\n",
    "   - ‚úÖ Use **out-of-sample** real-world data for validation.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Fixing Incorrect Log Loss Calculation**\n",
    "**‚ùì Question:**  \n",
    "You implemented **log loss** manually, but the result **differs from scikit-learn‚Äôs log loss**. Find and fix the bug.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_probs = np.array([0.9, 0.2, 0.7, 0.8, 0.3])\n",
    "\n",
    "log_loss = -np.mean(y_true * np.log(y_pred_probs) + (1 - y_true) * np.log(1 - y_pred_probs))\n",
    "print(\"Log Loss:\", log_loss)\n",
    "```\n",
    "\n",
    "### **‚úÖ Solution**\n",
    "üî¥ **Bug:** The implementation does **not handle zero probabilities**, causing **log(0) errors**.  \n",
    "‚úÖ **Fix:** Add `np.clip()` to prevent log(0):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_probs = np.array([0.9, 0.2, 0.7, 0.8, 0.3])\n",
    "\n",
    "# Fix: Avoid log(0) by clipping probabilities\n",
    "eps = 1e-15\n",
    "y_pred_probs = np.clip(y_pred_probs, eps, 1 - eps)\n",
    "\n",
    "log_loss_fixed = -np.mean(y_true * np.log(y_pred_probs) + (1 - y_true) * np.log(1 - y_pred_probs))\n",
    "print(\"Corrected Log Loss:\", log_loss_fixed)\n",
    "\n",
    "# Validate with sklearn\n",
    "print(\"Sklearn Log Loss:\", log_loss(y_true, y_pred_probs))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Debugging a Model with High Validation Loss**\n",
    "**‚ùì Question:**  \n",
    "Your model achieves **low training loss but high validation loss**. What could be wrong, and how would you fix it?\n",
    "\n",
    "### **‚úÖ Solution**\n",
    "The issue suggests **overfitting**. Possible fixes:\n",
    "\n",
    "1. **Increase Regularization (L2/L1 Penalty)**\n",
    "   ```python\n",
    "   model.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "   ```\n",
    "\n",
    "2. **Add Dropout**\n",
    "   ```python\n",
    "   model.add(keras.layers.Dropout(0.3))\n",
    "   ```\n",
    "\n",
    "3. **Use Early Stopping**\n",
    "   ```python\n",
    "   from tensorflow.keras.callbacks import EarlyStopping\n",
    "   early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "   ```\n",
    "\n",
    "4. **Ensure Proper Train-Test Splitting (Avoid Data Leakage)**\n",
    "   ```python\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Debugging Precision & Recall Mismatch**\n",
    "**‚ùì Question:**  \n",
    "Your model has **high precision but low recall**. What does this indicate, and how do you fix it?\n",
    "\n",
    "### **‚úÖ Solution**\n",
    "- **High Precision, Low Recall** = **Model is conservative** (only predicts positive when very confident).  \n",
    "- **Fix by adjusting the decision threshold**:\n",
    "  ```python\n",
    "  from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "  y_pred_probs = model.predict_proba(X_test)[:, 1]  # Get probabilities\n",
    "  precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)\n",
    "\n",
    "  # Find the threshold that gives a better balance\n",
    "  best_threshold = thresholds[np.argmax(recall >= 0.8)]  # Adjust recall target\n",
    "  y_pred_new = (y_pred_probs >= best_threshold).astype(int)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ Identifying Label Leakage in Feature Engineering**\n",
    "**‚ùì Question:**  \n",
    "Your model has **great validation performance** but fails **in production**. You suspect **data leakage**. How would you debug this?\n",
    "\n",
    "### **‚úÖ Solution**\n",
    "üîç **Check if Features Contain Target Information**  \n",
    "- **Feature leakage happens if a feature is correlated with the label in a way that wouldn't be available at inference time**.\n",
    "\n",
    "üîπ **Fix:** Run feature importance analysis using SHAP:\n",
    "```python\n",
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_train)\n",
    "\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "```\n",
    "üîπ **If a future-based feature is highly important**, it likely contains **leaked information**.\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ Why is Model Accuracy 100%?**\n",
    "**‚ùì Question:**  \n",
    "You trained a model, but it achieves **100% accuracy** even on the validation set. What could be wrong?\n",
    "\n",
    "### **‚úÖ Solution**\n",
    "Possible issues:\n",
    "1. **Data Leakage** ‚Äì Training and validation data overlap. Fix by checking for duplicate rows:\n",
    "   ```python\n",
    "   duplicates = X_train.merge(X_test, how='inner')\n",
    "   print(\"Duplicates:\", len(duplicates))\n",
    "   ```\n",
    "\n",
    "2. **Overly Simple Dataset** ‚Äì If the dataset is too easy, the model may memorize it.\n",
    "\n",
    "3. **Training on the Wrong Data** ‚Äì Check if `X_train` and `y_train` match in shape and indices:\n",
    "   ```python\n",
    "   print(X_train.shape, y_train.shape)\n",
    "   print(X_train.head(), y_train.head())\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Advanced Debugging Techniques**\n",
    "| Issue | Debugging Fix |\n",
    "|--------|-------------|\n",
    "| **Accuracy is high, but business metrics are low** | Check class imbalance, proper target variable |\n",
    "| **Model overfits (low train loss, high val loss)** | Add dropout, L2 regularization, early stopping |\n",
    "| **High precision but low recall** | Adjust decision threshold for recall |\n",
    "| **AUC-ROC calculation is incorrect** | Ensure probability predictions instead of binary labels |\n",
    "| **Model fails in production** | Check for data leakage (SHAP analysis) |\n",
    "| **Model achieves 100% accuracy** | Check for duplicated train/test data |\n",
    "\n",
    "Would you like **even harder debugging challenges** with **real-world ML system failures**? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
