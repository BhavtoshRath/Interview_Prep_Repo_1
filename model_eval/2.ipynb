{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are **8 advanced TensorFlow interview questions** focused on **Evaluating ML Model Performance**, along with solutions and code examples:  \n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ How would you optimize an ML model for both accuracy and inference speed?**  \n",
    "‚úÖ **Answer:**  \n",
    "To balance accuracy and inference speed:  \n",
    "- Use **model pruning** to remove unnecessary weights.  \n",
    "- Apply **quantization** (e.g., FP32 ‚Üí INT8) to reduce model size.  \n",
    "- Use **TensorRT** for optimized inference on GPUs.  \n",
    "- Reduce batch size and **enable batch processing** for efficiency.  \n",
    "- Convert the model to **TensorFlow Lite (TFLite)** for mobile deployment.  \n",
    "\n",
    "‚úÖ **Code Example (Converting Model to TFLite for Faster Inference)**  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert Keras model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable optimizations\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save optimized model\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "```\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Your ranking model's NDCG@10 dropped after adding a new feature. How do you debug?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Check feature importance**: The new feature may be noisy or redundant.  \n",
    "- **Analyze per-query performance**: Does NDCG drop across all queries or only specific cases?  \n",
    "- **Compare ranking distributions**: Plot histograms of scores before and after adding the feature.  \n",
    "- **Ablation testing**: Train two models (with and without the feature) and compare their rankings.  \n",
    "\n",
    "‚úÖ **Code Example (Feature Importance with SHAP)**  \n",
    "```python\n",
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test)  # Visualize feature importance\n",
    "```\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Your model has high precision but low recall. What does this mean, and how do you fix it?**  \n",
    "‚úÖ **Answer:**  \n",
    "- High precision but low recall means the model is **conservative** (few false positives, but many false negatives).  \n",
    "- Solutions:  \n",
    "  - Lower the **classification threshold** (e.g., from 0.9 to 0.7).  \n",
    "  - **Use recall-focused loss functions** (e.g., recall-weighted F1 loss).  \n",
    "  - **Balance precision vs. recall** using Precision-Recall curves.  \n",
    "\n",
    "‚úÖ **Code Example (Adjusting Classification Threshold)**  \n",
    "```python\n",
    "y_probs = model.predict(X_test)  # Get probability scores\n",
    "threshold = 0.7  # Lower threshold to increase recall\n",
    "y_preds = (y_probs > threshold).astype(int)\n",
    "```\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ How do you handle the cold start problem in Etsy Ads ranking?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Zero-shot learning**: Use pre-trained embeddings from similar products.  \n",
    "- **Hybrid models**: Combine content-based + collaborative filtering.  \n",
    "- **Heuristics**: Give new items an initial boost.  \n",
    "- **Few-shot learning**: Train on similar products using meta-learning.  \n",
    "\n",
    "‚úÖ **Example (Zero-shot Learning with Pretrained Embeddings)**  \n",
    "```python\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load universal sentence encoder for text embeddings\n",
    "embedding_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "new_item_embedding = embedding_model([\"handmade vintage necklace\"])\n",
    "```\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Your model's AUC-ROC is high, but AUC-PR is low. Why?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Class imbalance issue**: AUC-ROC can look high even if the model rarely predicts the minority class.  \n",
    "- **PR curve is more sensitive to false positives**: If precision is low, it signals that the model is making many incorrect positive predictions.  \n",
    "\n",
    "‚úÖ **Solution:**  \n",
    "- Use **precision-recall curve instead of ROC curve** for imbalanced data.  \n",
    "- Adjust **class weights** or use **Focal Loss**.  \n",
    "\n",
    "‚úÖ **Code Example (Plotting PR Curve in TensorFlow)**  \n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, model.predict(X_test))\n",
    "\n",
    "plt.plot(recall, precision, label=\"PR Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "---\n",
    "\n",
    "### **6Ô∏è‚É£ Your model's evaluation metrics are stable, but business KPIs (e.g., revenue) are dropping. Why?**  \n",
    "‚úÖ **Possible Reasons:**  \n",
    "- **Feature drift**: Model trained on outdated data.  \n",
    "- **User behavior change**: Model is optimizing for the wrong objective.  \n",
    "- **Non-stationarity**: Seasonal patterns affect ad performance.  \n",
    "- **Online vs. offline mismatch**: Offline evaluation may not reflect real user behavior.  \n",
    "\n",
    "‚úÖ **Solution:**  \n",
    "- Implement **real-time model monitoring** (track feature distributions).  \n",
    "- Conduct **A/B tests** to verify business impact.  \n",
    "\n",
    "‚úÖ **Example (Detecting Data Drift with TensorFlow Data Validation)**  \n",
    "```python\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "# Load and compare two datasets (old and new data)\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(train_data)\n",
    "test_stats = tfdv.generate_statistics_from_dataframe(test_data)\n",
    "\n",
    "# Compare distributions\n",
    "tfdv.visualize_statistics(lhs_statistics=train_stats, rhs_statistics=test_stats)\n",
    "```\n",
    "---\n",
    "\n",
    "### **7Ô∏è‚É£ How do you improve a model‚Äôs generalization when deployed to production?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Regularization**: Apply **dropout, L2 norm, or batch normalization**.  \n",
    "- **Data Augmentation**: Create variations of the input to make the model robust.  \n",
    "- **Ensemble Learning**: Combine multiple models to reduce variance.  \n",
    "- **Domain Adaptation**: Use transfer learning to adapt to different environments.  \n",
    "\n",
    "‚úÖ **Example (Adding Dropout & L2 Regularization in TensorFlow)**  \n",
    "```python\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # L2 Regularization\n",
    "    Dropout(0.3),  # Dropout added\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "---\n",
    "\n",
    "### **8Ô∏è‚É£ Your model‚Äôs inference time is too slow. How do you optimize it?**  \n",
    "‚úÖ **Solution:**  \n",
    "- **Quantization**: Convert weights to lower precision (e.g., INT8).  \n",
    "- **TensorRT Acceleration**: Optimize for GPU inference.  \n",
    "- **Batching & Caching**: Process multiple inputs at once.  \n",
    "\n",
    "‚úÖ **Example (Using TensorRT to Optimize Model for GPU Inference)**  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.experimental import tensorrt as trt\n",
    "\n",
    "# Convert Keras model to TensorRT-optimized model\n",
    "converter = trt.Converter(input_saved_model_dir=\"model\")\n",
    "converter.convert()\n",
    "\n",
    "# Save optimized model\n",
    "converter.save(\"optimized_model\")\n",
    "```\n",
    "---\n",
    "\n",
    "### üöÄ **Summary of Advanced Topics Covered:**  \n",
    "1. **Optimizing for both accuracy & inference speed (TFLite, TensorRT)**  \n",
    "2. **Debugging ranking models (NDCG drops, SHAP analysis)**  \n",
    "3. **Handling high precision but low recall (threshold tuning)**  \n",
    "4. **Cold start problem in ranking models (Zero-shot learning)**  \n",
    "5. **AUC-ROC vs. AUC-PR (Class imbalance issue)**  \n",
    "6. **Business metric misalignment (A/B testing, feature drift detection)**  \n",
    "7. **Improving generalization (Regularization, Data Augmentation)**  \n",
    "8. **Reducing inference time (Quantization, TensorRT, Caching)**  \n",
    "\n",
    "Would you like **more hands-on coding exercises** to practice these? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some **slightly harder TensorFlow interview questions** with solutions, focusing on evaluating machine learning model performance:\n",
    "\n",
    "---\n",
    "\n",
    "### **Harder Questions with Solutions**\n",
    "\n",
    "1. **How do you implement custom metrics in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     import tensorflow as tf\n",
    "\n",
    "     # Custom metric: F1-Score\n",
    "     class F1Score(tf.keras.metrics.Metric):\n",
    "         def __init__(self, name='f1_score', **kwargs):\n",
    "             super(F1Score, self).__init__(name=name, **kwargs)\n",
    "             self.precision = tf.keras.metrics.Precision()\n",
    "             self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "         def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "             self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "             self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "         def result(self):\n",
    "             p = self.precision.result()\n",
    "             r = self.recall.result()\n",
    "             return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "         def reset_states(self):\n",
    "             self.precision.reset_states()\n",
    "             self.recall.reset_states()\n",
    "\n",
    "     # Usage\n",
    "     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[F1Score()])\n",
    "     ```\n",
    "\n",
    "2. **How do you implement a custom callback to monitor a specific metric during training?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     class CustomCallback(tf.keras.callbacks.Callback):\n",
    "         def on_epoch_end(self, epoch, logs=None):\n",
    "             if logs.get('val_f1_score') > 0.8:  # Monitor F1-Score\n",
    "                 print(\"\\nReached 0.8 validation F1-Score! Stopping training.\")\n",
    "                 self.model.stop_training = True\n",
    "\n",
    "     # Usage\n",
    "     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, callbacks=[CustomCallback()])\n",
    "     ```\n",
    "\n",
    "3. **How do you implement a learning rate scheduler in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     def lr_scheduler(epoch, lr):\n",
    "         if epoch < 10:\n",
    "             return lr\n",
    "         else:\n",
    "             return lr * tf.math.exp(-0.1)\n",
    "\n",
    "     lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, callbacks=[lr_callback])\n",
    "     ```\n",
    "\n",
    "4. **How do you handle multi-label classification evaluation in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     # Use binary cross-entropy and sigmoid activation for multi-label classification\n",
    "     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(multi_label=True)])\n",
    "\n",
    "     # Evaluate\n",
    "     y_pred = model.predict(X_test)\n",
    "     y_pred_classes = (y_pred > 0.5).astype(int)  # Threshold predictions\n",
    "     ```\n",
    "\n",
    "5. **How do you implement a custom loss function in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     def custom_loss(y_true, y_pred):\n",
    "         # Example: Weighted MSE\n",
    "         error = y_true - y_pred\n",
    "         weights = tf.where(y_true > 0, 2.0, 1.0)  # Assign higher weight to positive labels\n",
    "         return tf.reduce_mean(weights * tf.square(error))\n",
    "\n",
    "     model.compile(optimizer='adam', loss=custom_loss)\n",
    "     ```\n",
    "\n",
    "6. **How do you implement stratified k-fold cross-validation in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "     for train_idx, val_idx in skf.split(X, y):\n",
    "         X_train, X_val = X[train_idx], X[val_idx]\n",
    "         y_train, y_val = y[train_idx], y[val_idx]\n",
    "         model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "     ```\n",
    "\n",
    "7. **How do you evaluate a model using bootstrapping in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     import numpy as np\n",
    "\n",
    "     n_bootstraps = 100\n",
    "     accuracies = []\n",
    "\n",
    "     for _ in range(n_bootstraps):\n",
    "         indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "         X_bootstrap = X_test[indices]\n",
    "         y_bootstrap = y_test[indices]\n",
    "         loss, accuracy = model.evaluate(X_bootstrap, y_bootstrap, verbose=0)\n",
    "         accuracies.append(accuracy)\n",
    "\n",
    "     print(\"Mean Accuracy:\", np.mean(accuracies))\n",
    "     print(\"Confidence Interval:\", np.percentile(accuracies, [2.5, 97.5]))\n",
    "     ```\n",
    "\n",
    "8. **How do you implement a custom evaluation loop in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "     accuracy_metric = tf.keras.metrics.Accuracy()\n",
    "\n",
    "     for X_batch, y_batch in test_dataset:\n",
    "         y_pred = model(X_batch)\n",
    "         y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "         accuracy_metric.update_state(y_batch, y_pred_classes)\n",
    "\n",
    "     print(\"Test Accuracy:\", accuracy_metric.result().numpy())\n",
    "     ```\n",
    "\n",
    "9. **How do you evaluate a model using precision-recall curves?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     from sklearn.metrics import precision_recall_curve\n",
    "     import matplotlib.pyplot as plt\n",
    "\n",
    "     y_pred = model.predict(X_test)\n",
    "     precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "     plt.plot(recall, precision, label='Precision-Recall Curve')\n",
    "     plt.xlabel('Recall')\n",
    "     plt.ylabel('Precision')\n",
    "     plt.title('Precision-Recall Curve')\n",
    "     plt.legend()\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "10. **How do you implement a custom evaluation metric for multi-class classification?**\n",
    "    - **Solution**:\n",
    "      ```python\n",
    "      class MulticlassF1Score(tf.keras.metrics.Metric):\n",
    "          def __init__(self, num_classes, name='multiclass_f1_score', **kwargs):\n",
    "              super(MulticlassF1Score, self).__init__(name=name, **kwargs)\n",
    "              self.num_classes = num_classes\n",
    "              self.precision = tf.keras.metrics.Precision()\n",
    "              self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "          def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "              y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=self.num_classes)\n",
    "              y_pred = tf.one_hot(tf.argmax(y_pred, axis=1), depth=self.num_classes)\n",
    "              self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "              self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "          def result(self):\n",
    "              p = self.precision.result()\n",
    "              r = self.recall.result()\n",
    "              return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "          def reset_states(self):\n",
    "              self.precision.reset_states()\n",
    "              self.recall.reset_states()\n",
    "\n",
    "      # Usage\n",
    "      model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[MulticlassF1Score(num_classes=3)])\n",
    "      ```\n",
    "\n",
    "11. **How do you evaluate a model using k-fold cross-validation with TensorFlow and Keras?**\n",
    "    - **Solution**:\n",
    "      ```python\n",
    "      from sklearn.model_selection import KFold\n",
    "      import numpy as np\n",
    "\n",
    "      kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "      accuracies = []\n",
    "\n",
    "      for train_idx, val_idx in kfold.split(X):\n",
    "          X_train, X_val = X[train_idx], X[val_idx]\n",
    "          y_train, y_val = y[train_idx], y[val_idx]\n",
    "          model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, verbose=0)\n",
    "          _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "          accuracies.append(accuracy)\n",
    "\n",
    "      print(\"Mean Accuracy:\", np.mean(accuracies))\n",
    "      print(\"Standard Deviation:\", np.std(accuracies))\n",
    "      ```\n",
    "\n",
    "12. **How do you evaluate a model using a custom threshold for binary classification?**\n",
    "    - **Solution**:\n",
    "      ```python\n",
    "      y_pred = model.predict(X_test)\n",
    "      custom_threshold = 0.6\n",
    "      y_pred_classes = (y_pred > custom_threshold).astype(int)\n",
    "\n",
    "      from sklearn.metrics import classification_report\n",
    "      print(classification_report(y_test, y_pred_classes))\n",
    "      ```\n",
    "\n",
    "---\n",
    "\n",
    "These harder questions and solutions dive deeper into advanced evaluation techniques, custom implementations, and practical scenarios for evaluating machine learning models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
