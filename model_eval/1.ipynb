{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some **easy to medium** TensorFlow interview questions focused on **Evaluating ML Model Performance**, along with answers and code snippets:  \n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ What are the common metrics for evaluating classification models?**\n",
    "‚úÖ **Answer:**  \n",
    "Common evaluation metrics include:  \n",
    "- **Accuracy**: `(TP + TN) / (TP + TN + FP + FN)`\n",
    "- **Precision**: `TP / (TP + FP)` (How many predicted positives were actually positive?)\n",
    "- **Recall**: `TP / (TP + FN)` (How many actual positives were detected?)\n",
    "- **F1-Score**: `2 * (Precision * Recall) / (Precision + Recall)`\n",
    "- **AUC-ROC**: Measures the trade-off between true positive rate and false positive rate.\n",
    "- **Log Loss**: Penalizes incorrect predictions with probability scores.\n",
    "\n",
    "‚úÖ **Code Example (Compute Classification Metrics in TensorFlow/Keras)**  \n",
    "```python\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "y_true = [0, 1, 1, 0, 1, 1, 0]\n",
    "y_pred = [0, 1, 0, 0, 1, 1, 1]  # Binary predictions\n",
    "\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "auc = AUC()\n",
    "\n",
    "precision.update_state(y_true, y_pred)\n",
    "recall.update_state(y_true, y_pred)\n",
    "auc.update_state(y_true, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision.result().numpy())\n",
    "print(\"Recall:\", recall.result().numpy())\n",
    "print(\"AUC Score:\", auc.result().numpy())\n",
    "```\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ How would you evaluate a ranking model for Etsy Ads?**\n",
    "‚úÖ **Answer:**  \n",
    "Ranking models require **ranking-specific metrics**, such as:  \n",
    "- **NDCG (Normalized Discounted Cumulative Gain)** ‚Äì Measures ranking quality while prioritizing relevant results at the top.\n",
    "- **Precision@K** ‚Äì Measures how many of the top K retrieved results are relevant.\n",
    "- **Mean Reciprocal Rank (MRR)** ‚Äì Evaluates how early in the ranking the first relevant result appears.\n",
    "- **CTR (Click-Through Rate)** ‚Äì Measures how often users click on the ads.\n",
    "\n",
    "‚úÖ **Code Example (Computing Precision@K with TensorFlow)**  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=3):\n",
    "    \"\"\"\n",
    "    Compute Precision@K for ranking.\n",
    "    y_true: Binary relevance labels (1 for relevant, 0 for non-relevant)\n",
    "    y_pred: Model scores for ranking (higher means more relevant)\n",
    "    k: Number of top results to consider\n",
    "    \"\"\"\n",
    "    indices = tf.argsort(y_pred, direction='DESCENDING')[:k]\n",
    "    top_k_relevance = tf.gather(y_true, indices)\n",
    "    return tf.reduce_sum(top_k_relevance) / tf.cast(k, tf.float32)\n",
    "\n",
    "y_true = tf.constant([0, 1, 1, 0, 1])  # Ground truth relevance\n",
    "y_pred = tf.constant([0.3, 0.7, 0.6, 0.2, 0.8])  # Predicted ranking scores\n",
    "\n",
    "print(\"Precision@3:\", precision_at_k(y_true, y_pred, k=3).numpy())\n",
    "```\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ How do you handle class imbalance in evaluation?**\n",
    "‚úÖ **Answer:**  \n",
    "Class imbalance means one class dominates the dataset, making accuracy unreliable. Strategies to handle it:\n",
    "- **Use balanced metrics**: Precision, Recall, F1-score instead of Accuracy.\n",
    "- **AUC-ROC & AUC-PR**: Work well for imbalanced datasets.\n",
    "- **Resampling**: Upsampling minority class or downsampling majority class.\n",
    "- **Use Class Weights**: Assign higher weight to the minority class.\n",
    "\n",
    "‚úÖ **Code Example (Using Class Weights in TensorFlow/Keras)**  \n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a simple model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(20,)),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define class weights (e.g., for 90% vs. 10% imbalance)\n",
    "class_weights = {0: 1.0, 1: 5.0}\n",
    "\n",
    "# Train with class weights\n",
    "model.fit(X_train, y_train, epochs=10, class_weight=class_weights, validation_data=(X_test, y_test))\n",
    "```\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Your TensorFlow model has high accuracy but low business metric (e.g., conversions). What could be wrong?**\n",
    "‚úÖ **Possible Reasons & Fixes:**\n",
    "- **Mismatch between ML metric and business goal**: Accuracy does not always reflect business success (e.g., CTR or revenue).\n",
    "- **Dataset bias**: Model may be overfitting to a dataset that doesn't reflect real users.\n",
    "- **Wrong evaluation strategy**: Metrics like Precision@K or AUC might be better than simple accuracy.\n",
    "- **Cold Start Problem**: Model might be bad at generalizing to new items.\n",
    "\n",
    "‚úÖ **Solution: Try Optimizing for Business Metrics**  \n",
    "```python\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])  # Instead of accuracy\n",
    "```\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ How do you know if a model is overfitting?**\n",
    "‚úÖ **Answer:**\n",
    "- **Training accuracy is high but validation accuracy is low**.\n",
    "- **Validation loss stops decreasing while training loss keeps dropping**.\n",
    "- **Generalization gap**: Large gap between train and test performance.\n",
    "\n",
    "‚úÖ **Code Example (Using Early Stopping & Dropout to Prevent Overfitting)**  \n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define model with dropout\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dropout(0.3),  # Dropout added\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "```\n",
    "---\n",
    "\n",
    "### **6Ô∏è‚É£ What is AUC-ROC and how is it different from AUC-PR?**\n",
    "‚úÖ **Answer:**\n",
    "- **AUC-ROC (Receiver Operating Characteristic)**: Measures trade-off between True Positive Rate and False Positive Rate.\n",
    "- **AUC-PR (Precision-Recall Curve)**: More useful when dealing with imbalanced datasets.\n",
    "\n",
    "‚úÖ **Code Example (Plotting AUC-ROC Curve)**  \n",
    "```python\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, model.predict(X_test))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "---\n",
    "\n",
    "### **7Ô∏è‚É£ What is the difference between Precision@K and NDCG?**\n",
    "‚úÖ **Answer:**\n",
    "- **Precision@K**: Measures how many of the top K results are relevant.\n",
    "- **NDCG (Normalized Discounted Cumulative Gain)**: Accounts for **position** of relevant results‚Äîranking order matters.\n",
    "\n",
    "---\n",
    "\n",
    "### **8Ô∏è‚É£ What are some techniques to debug a model with NaN loss values?**\n",
    "‚úÖ **Answer:**\n",
    "- **Check for numerical instability** (e.g., exploding gradients).\n",
    "- **Use `tf.debugging.check_numerics()`** to find problematic tensors.\n",
    "- **Reduce learning rate**.\n",
    "- **Clip gradients** to prevent extreme updates.\n",
    "\n",
    "‚úÖ **Example (Clipping Gradients to Prevent Exploding Values)**  \n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)  # Clips gradients\n",
    "```\n",
    "---\n",
    "\n",
    "These **8 easy-to-medium questions** will test your ability to evaluate and improve ML model performance using TensorFlow.\n",
    "\n",
    "Would you like me to generate some **harder TensorFlow performance questions**? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the **solutions** to the TensorFlow interview questions for evaluating machine learning model performance:\n",
    "\n",
    "---\n",
    "\n",
    "### **Easy-Level Questions with Solutions**\n",
    "\n",
    "1. **What are common metrics used to evaluate classification models?**\n",
    "   - **Solution**: \n",
    "     - **Accuracy**: (TP + TN) / (TP + TN + FP + FN)\n",
    "     - **Precision**: TP / (TP + FP)\n",
    "     - **Recall**: TP / (TP + FN)\n",
    "     - **F1-Score**: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "     - **ROC-AUC**: Area under the Receiver Operating Characteristic curve.\n",
    "\n",
    "2. **How do you calculate accuracy in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     import tensorflow as tf\n",
    "\n",
    "     # Using tf.keras.metrics.Accuracy\n",
    "     accuracy = tf.keras.metrics.Accuracy()\n",
    "     accuracy.update_state([0, 1, 1, 0], [0, 1, 0, 0])  # True labels, Predicted labels\n",
    "     print(\"Accuracy:\", accuracy.result().numpy())\n",
    "\n",
    "     # Using model.evaluate()\n",
    "     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "     loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "     print(\"Test Accuracy:\", accuracy)\n",
    "     ```\n",
    "\n",
    "3. **What is a confusion matrix, and how can you create one in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     import tensorflow as tf\n",
    "\n",
    "     # True labels and predicted labels\n",
    "     y_true = [0, 1, 1, 0, 1]\n",
    "     y_pred = [0, 1, 0, 0, 1]\n",
    "\n",
    "     # Create confusion matrix\n",
    "     confusion_matrix = tf.math.confusion_matrix(y_true, y_pred)\n",
    "     print(\"Confusion Matrix:\\n\", confusion_matrix.numpy())\n",
    "     ```\n",
    "\n",
    "4. **What is overfitting, and how can you detect it?**\n",
    "   - **Solution**: Overfitting occurs when a model learns the training data too well, including noise, and performs poorly on unseen data. Detect it by:\n",
    "     - Comparing training and validation loss/metrics.\n",
    "     - If training loss decreases but validation loss increases, the model is overfitting.\n",
    "\n",
    "5. **How do you split data into training and testing sets in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     from sklearn.model_selection import train_test_split\n",
    "\n",
    "     # Split data into 80% training and 20% testing\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "     ```\n",
    "\n",
    "6. **What is the purpose of a validation set?**\n",
    "   - **Solution**: A validation set is used to:\n",
    "     - Tune hyperparameters.\n",
    "     - Evaluate model performance during training without using the test set.\n",
    "\n",
    "7. **How do you monitor loss and accuracy during training in TensorFlow?** (BR)\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     # Use TensorBoard or History callback\n",
    "     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "     history = model.fit(train_data, train_labels, validation_data=(val_data, val_labels), \n",
    "                         epochs=10, callbacks=[tensorboard_callback])\n",
    "\n",
    "     # Access loss and accuracy from history\n",
    "     print(history.history['loss'])\n",
    "     print(history.history['accuracy'])\n",
    "     ```\n",
    "\n",
    "8. **What is the difference between precision and recall?**\n",
    "   - **Solution**:\n",
    "     - **Precision**: Measures how many predicted positives are actually positive.\n",
    "     - **Recall**: Measures how many actual positives are correctly predicted.\n",
    "\n",
    "---\n",
    "\n",
    "### **Medium-Level Questions with Solutions**\n",
    "\n",
    "1. **How do you implement cross-validation in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     from sklearn.model_selection import KFold\n",
    "     import numpy as np\n",
    "\n",
    "     # Define KFold\n",
    "     kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "     # Perform cross-validation\n",
    "     for train_idx, val_idx in kfold.split(X):\n",
    "         X_train, X_val = X[train_idx], X[val_idx]\n",
    "         y_train, y_val = y[train_idx], y[val_idx]\n",
    "         model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "     ```\n",
    "\n",
    "2. **What is the ROC curve, and how do you plot it in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     from sklearn.metrics import roc_curve, auc\n",
    "     import matplotlib.pyplot as plt\n",
    "\n",
    "     # Get predictions\n",
    "     y_pred = model.predict(X_test)\n",
    "\n",
    "     # Compute ROC curve\n",
    "     fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "     roc_auc = auc(fpr, tpr)\n",
    "\n",
    "     # Plot ROC curve\n",
    "     plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "     plt.xlabel('False Positive Rate')\n",
    "     plt.ylabel('True Positive Rate')\n",
    "     plt.title('ROC Curve')\n",
    "     plt.legend()\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "3. **How do you handle imbalanced datasets when evaluating model performance?**\n",
    "   - **Solution**:\n",
    "     - Use metrics like F1-Score, ROC-AUC, or precision-recall curves.\n",
    "     - Use class weights:\n",
    "       ```python\n",
    "       class_weights = {0: 1, 1: 10}  # Assign higher weight to the minority class\n",
    "       model.fit(X_train, y_train, class_weight=class_weights, epochs=10)\n",
    "       ```\n",
    "\n",
    "4. **What is the difference between `model.evaluate()` and `model.predict()`?**\n",
    "   - **Solution**:\n",
    "     - `model.evaluate()` computes loss and metrics on test data.\n",
    "     - `model.predict()` generates predictions for input data.\n",
    "\n",
    "5. **How do you calculate the F1-Score in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     from sklearn.metrics import f1_score\n",
    "\n",
    "     y_pred = model.predict(X_test)\n",
    "     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "     f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "     print(\"F1-Score:\", f1)\n",
    "     ```\n",
    "\n",
    "6. **What is early stopping, and how do you implement it in TensorFlow?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "         monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=[early_stopping])\n",
    "     ```\n",
    "\n",
    "7. **How do you interpret a high bias or high variance problem in model performance?**\n",
    "   - **Solution**:\n",
    "     - **High Bias**: Underfitting. Increase model complexity or train longer.\n",
    "     - **High Variance**: Overfitting. Use regularization, dropout, or more data.\n",
    "\n",
    "8. **How do you use TensorBoard to visualize model performance?**\n",
    "   - **Solution**:\n",
    "     ```python\n",
    "     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[tensorboard_callback])\n",
    "\n",
    "     # Run TensorBoard in terminal\n",
    "     # tensorboard --logdir=./logs\n",
    "     ```\n",
    "\n",
    "9. **What is the difference between micro and macro averaging in multi-class classification?**\n",
    "   - **Solution**:\n",
    "     - **Micro Averaging**: Aggregates contributions of all classes.\n",
    "     - **Macro Averaging**: Computes metric independently for each class and averages.\n",
    "\n",
    "10. **How do you calculate the AUC-ROC score in TensorFlow?**\n",
    "    - **Solution**:\n",
    "      ```python\n",
    "      auc_metric = tf.keras.metrics.AUC(curve='ROC')\n",
    "      auc_metric.update_state(y_test, y_pred)\n",
    "      print(\"AUC-ROC:\", auc_metric.result().numpy())\n",
    "      ```\n",
    "\n",
    "11. **How do you evaluate a regression model in TensorFlow?**\n",
    "    - **Solution**:\n",
    "      ```python\n",
    "      model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "      loss, mae = model.evaluate(X_test, y_test)\n",
    "      print(\"Mean Absolute Error:\", mae)\n",
    "      ```\n",
    "\n",
    "---\n",
    "\n",
    "These solutions provide practical implementations for evaluating machine learning model performance using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
