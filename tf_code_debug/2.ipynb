{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Debugging ML Code & Error Analysis (Interview Coding Questions with Solutions)**  \n",
    "\n",
    "#### **Q1: The model is training, but accuracy is stuck at random (e.g., ~10% for 10-class classification). What could be wrong?**\n",
    "##### **Problem Code:**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "train_data = np.random.rand(1000, 784)\n",
    "train_labels = np.random.rand(1000, 10)  # Labels should be categorical integers\n",
    "\n",
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=5)\n",
    "```\n",
    "\n",
    "##### **Issue:**\n",
    "- `train_labels` should contain **integer class labels** (e.g., 0 to 9) for `sparse_categorical_crossentropy`, but it contains floating-point numbers.\n",
    "- The model is treating the labels as regression targets instead of classification labels.\n",
    "\n",
    "##### **Solution:**\n",
    "Convert `train_labels` to integer class labels:\n",
    "```python\n",
    "train_labels = np.random.randint(0, 10, size=(1000,))  # Integer class labels\n",
    "```\n",
    "Now the model will correctly interpret labels and start training as expected.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2: The model's loss is NaN during training. How would you debug it?**\n",
    "##### **Problem Code:**\n",
    "```python\n",
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with high learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=10.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "##### **Issue:**\n",
    "- The **learning rate is too high** (`10.0` instead of a typical range like `0.001`).\n",
    "- This causes **exploding gradients**, leading to NaN values.\n",
    "- High lr- exploding; low lr- vanishing\n",
    "\n",
    "##### **Solution:**\n",
    "Reduce the learning rate to a reasonable value:\n",
    "```python\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "If the issue persists, check for NaNs in tensors:\n",
    "```python\n",
    "tf.debugging.check_numerics(model.trainable_variables[0], \"Check NaN values in weights\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3: The model is overfitting. What steps can you take to mitigate this?**\n",
    "##### **Problem Code:**\n",
    "```python\n",
    "# Define a deep model without regularization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=50)  # Too many epochs, leading to overfitting\n",
    "```\n",
    "\n",
    "##### **Issue:**\n",
    "- The model is **too deep** without **regularization**.\n",
    "- **Too many epochs** cause overfitting to the training data.\n",
    "\n",
    "##### **Solution:**\n",
    "Introduce **dropout** and **L2 regularization**:\n",
    "```python\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=l2(0.01), input_shape=(784,)),\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout to reduce overfitting\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Reduce epochs to prevent excessive training\n",
    "model.fit(train_data, train_labels, epochs=10)\n",
    "```\n",
    "These changes help **prevent overfitting** by reducing reliance on specific neurons and penalizing large weights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4: The model training is extremely slow. How do you speed it up?**\n",
    "##### **Problem Code:**\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training on CPU with large dataset\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=1)  # Batch size is too small!\n",
    "```\n",
    "\n",
    "##### **Issue:**\n",
    "- **Very small batch size (batch_size=1)** slows down training because it's inefficient on GPUs.\n",
    "- Training on CPU instead of GPU.\n",
    "\n",
    "##### **Solution:**\n",
    "1. Increase **batch size** to take advantage of parallel processing:\n",
    "```python\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=32)  # More efficient\n",
    "```\n",
    "2. Ensure you’re using a **GPU** if available:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "```\n",
    "3. Use **prefetching** to speed up data loading:\n",
    "```python\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q5: The model's validation accuracy is much lower than training accuracy. What does this indicate?**\n",
    "##### **Problem:**\n",
    "- If training accuracy is **98%**, but validation accuracy is **65%**, the model is likely **overfitting**.\n",
    "\n",
    "##### **Solution:**\n",
    "- **Early stopping:** Stop training when validation loss stops improving.\n",
    "```python\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(train_data, train_labels, epochs=50, validation_data=(val_data, val_labels), callbacks=[callback])\n",
    "```\n",
    "- **Data Augmentation:** Helps generalize to unseen data.\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "```\n",
    "- **Reduce model complexity:** Use a smaller architecture.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Q6: You get a `ResourceExhaustedError` during training. What is happening?**\n",
    "##### **Problem Code:**\n",
    "```python\n",
    "# Defining a very large model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100000, activation='relu', input_shape=(784,)),  # Too many neurons\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "##### **Issue:**\n",
    "- **Too many parameters** are being stored in memory, exhausting the GPU’s VRAM.\n",
    "\n",
    "##### **Solution:**\n",
    "- **Reduce model size:** Use fewer neurons or layers.\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "- **Reduce batch size:** A large batch size uses more memory.\n",
    "```python\n",
    "model.fit(train_data, train_labels, batch_size=16)  # Reduce batch size\n",
    "```\n",
    "- **Enable mixed precision training** for better memory efficiency:\n",
    "```python\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1: Your model has high precision but low recall. What could be the issue?**\n",
    "#### **Scenario:**\n",
    "You are building a binary classifier for fraud detection. The model achieves **95% precision** but only **40% recall**.  \n",
    "\n",
    "#### **Analysis:**\n",
    "- **High precision, low recall** means that the model is being **too conservative**, predicting fraud only when it’s extremely confident.\n",
    "- This results in **many false negatives** (fraud cases missed).  \n",
    "\n",
    "#### **Possible Causes & Solutions:**\n",
    "1. **Threshold Adjustment**  \n",
    "   - The default decision threshold (0.5) might be too high. Lowering it can improve recall.  \n",
    "   ```python\n",
    "   y_pred_probs = model.predict(X_test)\n",
    "   threshold = 0.3  # Reduce threshold to classify more cases as fraud\n",
    "   y_pred = (y_pred_probs > threshold).astype(int)\n",
    "   ```\n",
    "   \n",
    "2. **Class Imbalance**  \n",
    "   - If the dataset is imbalanced (fraud cases are rare), the model may favor the majority class.\n",
    "   - **Solution:** Use class weighting or oversampling.\n",
    "   ```python\n",
    "   from sklearn.utils.class_weight import compute_class_weight\n",
    "   class_weights = compute_class_weight('balanced', classes=[0,1], y=y_train)\n",
    "   model.fit(X_train, y_train, class_weight={0: class_weights[0], 1: class_weights[1]})\n",
    "   ```\n",
    "   \n",
    "3. **Change Loss Function**  \n",
    "   - Use **focal loss** instead of binary cross-entropy to focus on hard-to-classify samples.\n",
    "   ```python\n",
    "   import tensorflow_addons as tfa\n",
    "   model.compile(loss=tfa.losses.SigmoidFocalCrossEntropy(), optimizer='adam', metrics=['accuracy'])\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Your deep learning model is producing identical outputs for all inputs.**\n",
    "#### **Scenario:**\n",
    "You train a neural network, but when testing, it always outputs the **same predicted value** regardless of the input.\n",
    "\n",
    "#### **Possible Causes & Fixes:**\n",
    "1. **Vanishing Gradient (All Outputs Converge to Same Value)**\n",
    "   - If all activations saturate, gradients become too small for learning.\n",
    "   - **Solution:** Switch activation functions.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.Dense(128, activation='relu'))  # Replace sigmoid/tanh with ReLU\n",
    "   ```\n",
    "\n",
    "2. **High L2 Regularization (Weights Shrink Too Much)**\n",
    "   - If L2 regularization is too strong, weights converge to near-zero.\n",
    "   - **Solution:** Reduce regularization strength.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "   ```\n",
    "\n",
    "3. **Batch Normalization Issues**\n",
    "   - If BatchNorm is misconfigured, it can cause the model to produce constant outputs.\n",
    "   - **Solution:** Ensure BatchNorm is properly placed.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.BatchNormalization())\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3: Your model is not improving beyond a certain accuracy after several epochs.**\n",
    "#### **Scenario:**\n",
    "You train a model for **image classification**, but its accuracy **plateaus at 70%**, despite additional training.\n",
    "\n",
    "#### **Possible Causes & Fixes:**\n",
    "1. **Learning Rate Too High or Too Low**\n",
    "   - **Fix:** Use a learning rate scheduler to adjust it dynamically.\n",
    "   ```python\n",
    "   lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "   model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[lr_scheduler])\n",
    "   ```\n",
    "\n",
    "2. **Insufficient Model Capacity**\n",
    "   - If the model is too simple, it may not have enough parameters to capture complexity.\n",
    "   - **Fix:** Add more layers or units.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "   ```\n",
    "\n",
    "3. **Data Augmentation**\n",
    "   - More diverse training data can help the model generalize better.\n",
    "   ```python\n",
    "   datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.2, horizontal_flip=True)\n",
    "   datagen.fit(X_train)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4: Your model performs well on training but poorly on validation/test data (overfitting).**\n",
    "#### **Scenario:**\n",
    "Your model achieves **98% accuracy on training** but **60% on validation**.\n",
    "\n",
    "#### **Possible Causes & Fixes:**\n",
    "1. **Dropout**\n",
    "   - Add dropout layers to prevent reliance on specific features.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.Dropout(0.5))\n",
    "   ```\n",
    "\n",
    "2. **Reduce Model Complexity**\n",
    "   - A too-large model memorizes training data but fails to generalize.\n",
    "   - **Fix:** Reduce the number of layers or neurons.\n",
    "   ```python\n",
    "   model = tf.keras.Sequential([\n",
    "       tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "       tf.keras.layers.Dense(10, activation='softmax')\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "3. **Data Augmentation & Early Stopping**\n",
    "   - Prevent overfitting by stopping training early.\n",
    "   ```python\n",
    "   early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Q5: The model's predictions are biased towards a particular class.**\n",
    "#### **Scenario:**\n",
    "Your multi-class model **always predicts class 0**, even though multiple classes exist.\n",
    "\n",
    "#### **Possible Causes & Fixes:**\n",
    "1. **Class Imbalance**\n",
    "   - Some classes might be underrepresented in the dataset.\n",
    "   - **Fix:** Resample or use class weighting.\n",
    "   ```python\n",
    "   from imblearn.over_sampling import SMOTE\n",
    "   smote = SMOTE()\n",
    "   X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "2. **Wrong Activation Function in Output Layer**\n",
    "   - If the final layer uses `softmax`, but loss function is `binary_crossentropy`, the model behaves incorrectly.\n",
    "   - **Fix:** Ensure correct configuration.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6: Model takes too long to train on large datasets.**\n",
    "#### **Scenario:**\n",
    "Your dataset has **millions of samples**, and training takes **hours per epoch**.\n",
    "\n",
    "#### **Possible Causes & Fixes:**\n",
    "1. **Use `tf.data` API for Efficient Loading**\n",
    "   ```python\n",
    "   train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "   train_dataset = train_dataset.batch(32).shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "   ```\n",
    "\n",
    "2. **Mixed Precision Training**\n",
    "   - Speed up training using float16 precision.\n",
    "   ```python\n",
    "   from tensorflow.keras.mixed_precision import set_global_policy\n",
    "   set_global_policy('mixed_float16')\n",
    "   ```\n",
    "\n",
    "3. **Use TensorFlow Lite for Deployment**\n",
    "   ```python\n",
    "   converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "   tflite_model = converter.convert()\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7: The model’s loss increases instead of decreasing during training.**\n",
    "#### **Scenario:**\n",
    "Instead of decreasing, your loss **keeps increasing** over epochs.\n",
    "\n",
    "#### **Possible Causes & Fixes:**\n",
    "1. **Too High Learning Rate**\n",
    "   - Loss might diverge due to large weight updates.\n",
    "   - **Fix:** Reduce learning rate.\n",
    "   ```python\n",
    "   optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "   ```\n",
    "\n",
    "2. **Exploding Gradients**\n",
    "   - **Fix:** Gradient clipping.\n",
    "   ```python\n",
    "   optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
    "   ```\n",
    "\n",
    "3. **Incorrect Loss Function**\n",
    "   - Example: Using `binary_crossentropy` for multi-class classification.\n",
    "   - **Fix:** Ensure correct loss.\n",
    "   ```python\n",
    "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are **5 intermediate-level debugging ML code & error analysis questions** with solutions:  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q1: Your TensorFlow model trains successfully but fails during inference (runtime error).**  \n",
    "#### **Scenario:**  \n",
    "You trained a model successfully, but when running inference, it **throws a shape mismatch error**.  \n",
    "\n",
    "#### **Possible Causes & Fixes:**  \n",
    "1. **Different Input Shape During Training & Inference**  \n",
    "   - **Fix:** Ensure input shapes match between training and inference.\n",
    "   ```python\n",
    "   print(\"Training Input Shape:\", X_train.shape)\n",
    "   print(\"Inference Input Shape:\", X_test.shape)\n",
    "   ```\n",
    "\n",
    "2. **Batch Normalization Behavior Changes Between Training & Inference**  \n",
    "   - **Fix:** Ensure `model.eval()` in PyTorch or `training=False` in Keras.\n",
    "   ```python\n",
    "   preds = model(X_test, training=False)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Model converges slowly or not at all.**  \n",
    "#### **Scenario:**  \n",
    "Your deep learning model **trains very slowly** or **fails to converge** after multiple epochs.  \n",
    "\n",
    "#### **Possible Causes & Fixes:**  \n",
    "1. **Learning Rate Too High or Too Low**  \n",
    "   - **Fix:** Use **learning rate scheduling**.\n",
    "   ```python\n",
    "   lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "       initial_learning_rate=0.01,\n",
    "       decay_steps=1000,\n",
    "       decay_rate=0.9)\n",
    "   optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "   ```\n",
    "\n",
    "2. **Weight Initialization Issue**  \n",
    "   - **Fix:** Use **He/Xavier initialization**.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "   ```\n",
    "\n",
    "3. **Gradient Vanishing in Deep Networks**  \n",
    "   - **Fix:** Use **Batch Normalization**.*\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.BatchNormalization())\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3: Your model works well on training data but overfits on validation.**  \n",
    "#### **Scenario:**  \n",
    "Your **training accuracy is high**, but **validation accuracy is low**.  \n",
    "\n",
    "#### **Possible Causes & Fixes:**  \n",
    "1. **Lack of Regularization**  \n",
    "   - **Fix:** Add **Dropout** or **L2 regularization**.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.Dropout(0.3))\n",
    "   model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "   ```\n",
    "\n",
    "2. **Data Augmentation Missing**  \n",
    "   - **Fix:** Use `ImageDataGenerator` for image tasks.\n",
    "   ```python\n",
    "   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "   datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
    "   ```\n",
    "\n",
    "3. **Reduce Model Complexity**  \n",
    "   - **Fix:** Reduce number of layers or parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4: Your multi-class classification model predicts only one class.**  \n",
    "#### **Scenario:**  \n",
    "Your model **always predicts the same class**, even though there are multiple classes.  \n",
    "\n",
    "#### **Possible Causes & Fixes:**  \n",
    "1. **Imbalanced Dataset**  \n",
    "   - **Fix:** Use **class weights** to balance training.\n",
    "   ```python\n",
    "   from sklearn.utils.class_weight import compute_class_weight\n",
    "   class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "   model.fit(X_train, y_train, class_weight=dict(enumerate(class_weights)))\n",
    "   ```\n",
    "\n",
    "2. **Softmax Activation Issue**  \n",
    "   - **Fix:** Ensure softmax is used for multi-class classification.\n",
    "   ```python\n",
    "   model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "   ```\n",
    "\n",
    "3. **Cross-Entropy Loss Issue**  \n",
    "   - **Fix:** Use `sparse_categorical_crossentropy` if labels are integers.\n",
    "   ```python\n",
    "   model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5: Your deep learning model runs out of memory (OOM error).**  \n",
    "#### **Scenario:**  \n",
    "When training a deep learning model, you **get an Out-Of-Memory (OOM) error**.  \n",
    "\n",
    "#### **Possible Causes & Fixes:**  \n",
    "1. **Batch Size Too Large**  \n",
    "   - **Fix:** Reduce batch size.\n",
    "   ```python\n",
    "   model.fit(X_train, y_train, batch_size=16)\n",
    "   ```\n",
    "\n",
    "2. **Use Mixed Precision Training**  \n",
    "   - **Fix:** Enable `float16` precision to reduce memory usage.\n",
    "   ```python\n",
    "   from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "   policy = mixed_precision.Policy('mixed_float16')\n",
    "   mixed_precision.set_policy(policy)\n",
    "   ```\n",
    "\n",
    "3. **Use Gradient Checkpointing** (for large Transformer models)  \n",
    "   ```python\n",
    "   model = tf.keras.models.Model(inputs, outputs)\n",
    "   model = tf.recompute_grad(model)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Thoughts**  \n",
    "These **intermediate debugging questions** test:  \n",
    "✅ **Shape mismatches**  \n",
    "✅ **Convergence issues**  \n",
    "✅ **Overfitting solutions**  \n",
    "✅ **Handling class imbalance**  \n",
    "✅ **Memory optimization**  \n",
    "\n",
    "Want more **company-specific ML debugging questions**? Let me know! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
