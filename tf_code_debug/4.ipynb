{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some easy to medium interview questions related to reading and modifying TensorFlow code, along with their solutions. These questions are designed to test your understanding of TensorFlow concepts, debugging skills, and ability to modify existing code.\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 1: Debugging a TensorFlow Model**\n",
    "You are given the following TensorFlow code for a simple neural network. The code is not working as expected. Identify the issue and fix it.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10)  # No activation specified\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((1000, 32))\n",
    "y_train = tf.random.uniform((1000,), maxval=10, dtype=tf.int32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "```\n",
    "\n",
    "#### **Issue**:\n",
    "The output layer (`layers.Dense(10)`) does not have an activation function. For a classification task with `sparse_categorical_crossentropy` loss, the output layer should use a `softmax` activation.\n",
    "\n",
    "#### **Fix**:\n",
    "Add `activation='softmax'` to the output layer.\n",
    "\n",
    "```python\n",
    "# Corrected model definition\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Add softmax activation\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 2: Modify a Model for Regression**\n",
    "You are given a TensorFlow model designed for binary classification. Modify it for a regression task.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Original model for binary classification\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "#### **Modification**:\n",
    "For regression, the `output layer should have no activation function (or a linear activation)`, and the loss function should be changed to `mean_squared_error` or another regression loss.\n",
    "\n",
    "```python\n",
    "# Modified model for regression\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # No activation for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])  # Use MAE as a metric\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Question 4: Change the Optimizer**\n",
    "You are given a TensorFlow model using the `adam` optimizer. Modify the code to use the `SGD` optimizer with a learning rate of `0.01`.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Original model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "#### **Solution**:\n",
    "Replace `optimizer='adam'` with `optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)`.\n",
    "\n",
    "```python\n",
    "# Modified model with SGD optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 5: Add Callbacks/ SAVE THE MODEL**\n",
    "You are given a TensorFlow model. Add a callback to save the best model during training based on validation accuracy.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((1000, 32))\n",
    "y_train = tf.random.uniform((1000,), maxval=10, dtype=tf.int32)\n",
    "X_val = tf.random.normal((200, 32))\n",
    "y_val = tf.random.uniform((200,), maxval=10, dtype=tf.int32)\n",
    "```\n",
    "\n",
    "#### **Solution**:\n",
    "Add a `ModelCheckpoint` callback.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.h5',  # Save the best model to this file\n",
    "    monitor='val_accuracy',     # Monitor validation accuracy\n",
    "    save_best_only=True,        # Save only the best model\n",
    "    mode='max'                  # Maximize validation accuracy\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[checkpoint_callback])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "These questions cover common TensorFlow tasks such as debugging, modifying models, adding layers, changing optimizers, and using callbacks. Practicing these will help you become comfortable with reading and modifying TensorFlow code in interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some **easy to medium-level interview questions** focused on **reading and modifying existing TensorFlow code** with solutions.\n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ Fix the Incorrect Activation Function**\n",
    "**Question:**  \n",
    "The following model is a simple MLP, but there is a mistake in the activation function of the output layer. Fix it to be suitable for a **multi-class classification task**.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(20,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='sigmoid')  # ‚ùå Incorrect activation function\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "**Solution:**  \n",
    "For multi-class classification, the output activation should be **softmax**, not **sigmoid**.\n",
    "```python\n",
    "# Fix: Change sigmoid to softmax\n",
    "model.layers[-1].activation = tf.keras.activations.softmax\n",
    "```\n",
    "Or modify directly:\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(20,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # ‚úÖ Fixed activation function\n",
    "])\n",
    "```\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Identify and Fix a Training Issue** (BR)\n",
    "**Question:**  \n",
    "The following code defines and trains a model but throws an error. Identify and fix the issue.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train = tf.random.normal((100, 10))\n",
    "y_train = tf.random.uniform((100,), maxval=2, dtype=tf.int32)  # Binary labels\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "The **categorical_crossentropy** loss function expects **one-hot encoded labels**, but `y_train` contains integer labels.  \n",
    "\n",
    "**categorical_crossentropy: One-hot encoding; sparse_categorical_crossentropy: Integers**\n",
    "\n",
    "**Fix 1:** Use **sparse_categorical_crossentropy** for integer labels.\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "**Fix 2:** Convert `y_train` to one-hot encoding:\n",
    "```python\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Modify Code to Use Dropout Regularization**\n",
    "**Question:**  \n",
    "Modify the following code to include **Dropout** layers to reduce overfitting.\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(50,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "**Solution:**  \n",
    "Insert **Dropout layers** after the dense layers.\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(50,)),\n",
    "    Dropout(0.3),  # Dropout added\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),  # Dropout added\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Modify the Model to Use Batch Normalization**\n",
    "**Question:**  \n",
    "Modify the following model to include **Batch Normalization** after the hidden layers.\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(100,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "**Solution:**  \n",
    "Add **BatchNormalization layers** after each dense layer (before activation).\n",
    "```python\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, use_bias=False),  # Remove bias when using BatchNorm\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),  # Activation after batch norm\n",
    "    Dense(32, use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "**Why Should We Remove Bias Before Batch Normalization?**  \n",
    "\n",
    "When using Batch Normalization (BN) in a neural network, it is recommended to remove the bias term (i.e., use_bias=False in layers like Dense or Conv2D). This is because the batch normalization operation itself eliminates the effect of biases.\n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Debug Model Input Shape Mismatch**\n",
    "**Question:**  \n",
    "The following model is producing an error during training. Fix it.\n",
    "\n",
    "```python\n",
    "X_train = tf.random.normal((100, 50))\n",
    "y_train = tf.random.uniform((100, 1), maxval=10, dtype=tf.int32)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(20,)),  # ‚ùå Incorrect input shape\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "The `input_shape=(20,)` does not match `X_train.shape[1]`, which is **50**.\n",
    "\n",
    "**Fix:** Change input shape to `input_shape=(50,)`\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(50,)),  # ‚úÖ Fixed input shape\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "---\n",
    "\n",
    "### **6Ô∏è‚É£ Add Early Stopping to Prevent Overfitting**\n",
    "**Question:**  \n",
    "Modify the training process to **stop early** if validation loss does not improve for **3 consecutive epochs**.\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n",
    "```\n",
    "**Solution:**  \n",
    "Use the **EarlyStopping** callback.\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "```\n",
    "---\n",
    "\n",
    "### **7Ô∏è‚É£ Convert the Model to Use Functional API** (BR)\n",
    "**Question:**  \n",
    "Convert the following **Sequential API** model into the **Functional API**.\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(30,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "**Solution:**  \n",
    "Use `tf.keras.Model` with the Functional API.\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "inputs = Input(shape=(30,))\n",
    "x = Dense(128, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "- Use the Sequential API when you need a quick and easy way to build a simple, linear stack of layers.\n",
    "- Use the Functional API when you need more flexibility, such as handling multiple inputs, multiple outputs, or complex architectures with shared layers or residual connections.\n",
    "\n",
    "---\n",
    "\n",
    "### **8Ô∏è‚É£ Modify Model to Use L2 Regularization**\n",
    "**Question:**  \n",
    "Modify the model to use **L2 weight regularization**.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(20,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "**Solution:**  \n",
    "Use `kernel_regularizer=tf.keras.regularizers.l2(0.01)`.\n",
    "```python\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01), input_shape=(20,)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "### **Final Thoughts:**\n",
    "These questions help you **understand, debug, and modify** existing TensorFlow code, which is essential for **real-world applications**. Would you like more advanced TensorFlow challenges? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some **advanced TensorFlow interview questions** that test your understanding of deep learning concepts, custom model building, and advanced TensorFlow features. These questions require a deeper knowledge of TensorFlow and machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 1: Custom Loss Function** (BR)\n",
    "You are tasked with implementing a custom loss function for a regression model. The loss function is the **Huber loss**, which is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{Huber Loss} = \n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y_{\\text{true}} - y_{\\text{pred}})^2 & \\text{if } |y_{\\text{true}} - y_{\\text{pred}}| \\leq \\delta \\\\\n",
    "\\delta \\cdot (|y_{\\text{true}} - y_{\\text{pred}}| - \\frac{1}{2} \\delta) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Implement this loss function in TensorFlow and use it to train a model.\n",
    "\n",
    "#### **Solution**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the Huber loss function\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) <= delta\n",
    "    squared_loss = 0.5 * tf.square(error)\n",
    "    linear_loss = delta * (tf.abs(error) - 0.5 * delta)\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# Define a simple regression model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer='adam', loss=huber_loss)\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((1000, 10))\n",
    "y_train = tf.random.normal((1000, 1))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "```\n",
    "\n",
    "**Difference between `tf.keras.layers.Dense(1)` and `tf.keras.layers.Dense(2)` in final layer?**  \n",
    "When the number of output neurons is 2, the model outputs a probability distribution over the 2 classes, where the probabilities are normalized to add up to 1. This is suitable for binary classification problems where the classes are mutually exclusive.\n",
    "\n",
    "When the number of output neurons is 1, the model outputs a single probability value, which represents the probability of the positive class. The probability of the negative class is implicitly 1 minus the output value. This is also suitable for binary classification problems, but the output interpretation is slightly different.\n",
    "\n",
    "\n",
    "\n",
    "When the number of output neurons is 1, it can also be used for regression problems, not just binary classification. In this case, the output is a continuous value, not a probability, and the activation function is typically linear (e.g. `activation='linear'` or no activation function at all), not sigmoid or softmax.\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 2: Custom Layer** (BR)\n",
    "Create a custom TensorFlow layer that applies a **learnable scaling factor** to its inputs. The layer should have a single trainable weight, which scales the input.\n",
    "\n",
    "#### **Solution**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Define the custom layer\n",
    "class ScalingLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        self.scale = self.add_weight(name='scale', shape=(1,), initializer='ones', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.scale\n",
    "\n",
    "# Use the custom layer in a model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    ScalingLayer(),  # Custom scaling layer\n",
    "    tf.keras.layers.Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "X_train = tf.random.normal((1000, 10))\n",
    "y_train = tf.random.normal((1000, 1))\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 3: Multi-Input Model**\n",
    "You are building a model that takes **two inputs**: a numerical feature vector and an image. The model should combine these inputs and produce a single output. Implement this in TensorFlow.\n",
    "\n",
    "#### **Solution**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, Concatenate\n",
    "\n",
    "# Define the inputs\n",
    "numerical_input = Input(shape=(10,), name='numerical_input')\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "\n",
    "# Process the numerical input\n",
    "x1 = Dense(64, activation='relu')(numerical_input)\n",
    "\n",
    "# Process the image input\n",
    "x2 = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(64, activation='relu')(x2)\n",
    "\n",
    "# Concatenate the two inputs\n",
    "combined = Concatenate()([x1, x2])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Model(inputs=[numerical_input, image_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data\n",
    "X_num = tf.random.normal((1000, 10))\n",
    "X_img = tf.random.normal((1000, 64, 64, 3))\n",
    "y = tf.random.uniform((1000,), maxval=2, dtype=tf.int32)\n",
    "\n",
    "# Train the model\n",
    "model.fit({'numerical_input': X_num, 'image_input': X_img}, y, epochs=10)\n",
    "```\n",
    "\n",
    "**Why Can't We Use Pure Sequential API?**  \n",
    "Sequential only supports a single input; it doesn't handle multiple inputs natively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 5: Transfer Learning with Fine-Tuning**\n",
    "You are given a pre-trained `ResNet50` model. Fine-tune the model on a new dataset by freezing the initial layers and training only the last few layers.\n",
    "\n",
    "#### **Solution**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Load the pre-trained ResNet50 model without the top classification layer\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data\n",
    "X_train = tf.random.normal((1000, 224, 224, 3))\n",
    "y_train = tf.one_hot(tf.random.uniform((1000,), maxval=10, dtype=tf.int32), depth=10)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "# Unfreeze some layers for fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "These advanced questions test your ability to:\n",
    "1. Implement custom loss functions and layers.\n",
    "2. Build multi-input models.\n",
    "3. Use `GradientTape` for custom training loops.\n",
    "4. Perform transfer learning and fine-tuning.\n",
    "\n",
    "Practicing these will help you tackle complex TensorFlow tasks in interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are **10 advanced TensorFlow coding interview questions** with solutions that focus on **debugging, optimizing, and modifying existing TensorFlow code** in real-world scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Debugging a Failing Model Training (Exploding Gradients)**\n",
    "**Question:**  \n",
    "Your deep neural network is not converging, and the loss is increasing. Modify the following model to **prevent exploding gradients**.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', input_shape=(100,)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "Use **gradient clipping** to prevent exploding gradients.\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)  # Clipping gradients\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Implementing a Custom Loss Function**\n",
    "**Question:**  \n",
    "Write a custom loss function in TensorFlow that **penalizes incorrect predictions more strongly when confidence is high**.\n",
    "\n",
    "**Solution:**  \n",
    "Use **confidence-weighted cross-entropy loss**.\n",
    "\n",
    "```python\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def confidence_penalized_loss(y_true, y_pred):\n",
    "    confidence = K.max(y_pred, axis=-1)  # Get max probability per sample\n",
    "    loss = K.categorical_crossentropy(y_true, y_pred) * confidence  # Scale by confidence\n",
    "    return loss\n",
    "\n",
    "model.compile(optimizer='adam', loss=confidence_penalized_loss, metrics=['accuracy'])\n",
    "```\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Debugging Vanishing Gradients in Deep Networks**\n",
    "**Question:**  \n",
    "Your deep network is not learning well. Modify the following code to **fix the vanishing gradient problem**.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='sigmoid', input_shape=(100,)),\n",
    "    tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "Replace **sigmoid** with **ReLU** and use **Batch Normalization**.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, use_bias=False),  \n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(256, use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Optimize Model with Mixed Precision Training**\n",
    "**Question:**  \n",
    "Modify the training process to use **Mixed Precision Training** for better speed and efficiency.\n",
    "\n",
    "**Solution:**  \n",
    "Enable **mixed precision** with `tf.keras.mixed_precision.set_global_policy`.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "set_global_policy('mixed_float16')  # Enable mixed precision\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ Debugging Incorrect Image Preprocessing Pipeline**\n",
    "**Question:**  \n",
    "The following image preprocessing pipeline results in **NaN values** during training. Fix the issue.\n",
    "\n",
    "```python\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/0)  # ‚ùå Incorrect\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "Fix **division by zero error**:\n",
    "\n",
    "```python\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)  # ‚úÖ Corrected\n",
    "```\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ Implementing a Custom Metric (F1 Score)**\n",
    "**Question:**  \n",
    "TensorFlow does not have a built-in **F1 score** metric. Implement a **custom F1 score**.\n",
    "\n",
    "**Solution:**  \n",
    "Use `precision` and `recall` to compute **F1 score**.\n",
    "\n",
    "```python\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(y_true * y_pred)\n",
    "    precision = tp / (K.sum(y_pred) + K.epsilon())\n",
    "    recall = tp / (K.sum(y_true) + K.epsilon())\n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_score])\n",
    "```\n",
    "---\n",
    "\n",
    "## **7Ô∏è‚É£ Fixing TensorFlow Dataset Prefetching for Performance**\n",
    "**Question:**  \n",
    "Improve the efficiency of the following TensorFlow dataset pipeline.\n",
    "\n",
    "```python\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.shuffle(100)\n",
    "dataset = dataset.map(process_fn)\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "Use `prefetch()` for better performance.\n",
    "\n",
    "```python\n",
    "dataset = dataset.shuffle(100).batch(32).map(process_fn).prefetch(tf.data.AUTOTUNE)  # ‚úÖ Optimized\n",
    "```\n",
    "---\n",
    "\n",
    "## **8Ô∏è‚É£ Debugging Model Serialization Error**\n",
    "**Question:**  \n",
    "The following model fails to save. Fix the issue.\n",
    "\n",
    "```python\n",
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n",
    "\n",
    "model = tf.keras.Sequential([CustomLayer(64), tf.keras.layers.Dense(10, activation='softmax')])\n",
    "model.save(\"model.h5\")  # ‚ùå Throws error\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "Use a **custom serialization method**.\n",
    "\n",
    "```python\n",
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units}\n",
    "\n",
    "model.save(\"model.h5\")  # ‚úÖ Now it works\n",
    "```\n",
    "---\n",
    "\n",
    "## **9Ô∏è‚É£ Converting a Model to TensorFlow Lite**\n",
    "**Question:**  \n",
    "Convert a trained TensorFlow model to **TensorFlow Lite**.\n",
    "\n",
    "**Solution:**  \n",
    "Use `TFLiteConverter`.\n",
    "\n",
    "```python\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "```\n",
    "---\n",
    "\n",
    "## **üîü Implementing a Custom Training Loop with Gradient Tape**\n",
    "**Question:**  \n",
    "Write a **custom training loop** using `tf.GradientTape`.\n",
    "\n",
    "**Solution:**  \n",
    "Manually compute gradients and apply updates.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for X_batch, y_batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.numpy()}\")\n",
    "```\n",
    "\n",
    "5. **Why Use `GradientTape`?**\n",
    "   - **Automatic Differentiation**: TensorFlow uses `GradientTape` to **automatically compute gradients** through backpropagation, without needing to manually calculate derivatives.\n",
    "   - **Custom Training Loop**: Using `GradientTape` allows for more **flexibility** in defining a custom training loop. You can modify the training process, optimize specific parameters, or implement advanced algorithms that TensorFlow‚Äôs `model.fit()` function may not directly support.\n",
    "   - **Efficiency**: TensorFlow optimizes the calculation of gradients, making it faster than manually calculating them. It uses the recorded operations to efficiently compute gradients and apply them to the model's parameters.\n",
    "---\n",
    "\n",
    "### **üî• Final Thoughts**\n",
    "These **advanced TensorFlow coding interview questions** cover:\n",
    "‚úî Debugging  \n",
    "‚úî Model Optimization  \n",
    "‚úî Custom Loss & Metrics  \n",
    "‚úî Performance Tuning  \n",
    "‚úî TensorFlow Lite Conversion  \n",
    "‚úî Custom Training Loops  \n",
    "\n",
    "Would you like **more hands-on problems** or a **mock interview-style quiz**? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
