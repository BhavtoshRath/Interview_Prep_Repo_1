{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are **8 easy to medium TensorFlow interview questions** focused on **Handling Edge Cases & Model Robustness**, along with answers and code examples.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ How do you handle missing values in a dataset before training a TensorFlow model?**  \n",
    "‚úÖ **Answer:**  \n",
    "Missing values can cause model failures or degraded performance. Solutions include:  \n",
    "- **Remove missing values** if the dataset is large enough.  \n",
    "- **Impute missing values** using mean, median, or deep learning techniques.  \n",
    "- **Use a separate feature** to indicate missing data (binary flag).  \n",
    "\n",
    "‚úÖ **Code Example (Impute Missing Values with Mean)**  \n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace NaN values with the column mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Your model gives incorrect predictions on rare categories (class imbalance). How do you fix this?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Use balanced loss functions** (e.g., focal loss, weighted cross-entropy).  \n",
    "- **Oversample rare classes** using SMOTE or data augmentation.  \n",
    "- **Downsample frequent classes** to balance the dataset.  \n",
    "- **Generate synthetic data** for underrepresented classes.  \n",
    "\n",
    "‚úÖ **Code Example (Use Class Weights to Balance Training)**  \n",
    "```python\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train model with class weights\n",
    "model.fit(X_train, y_train, epochs=10, class_weight=class_weights_dict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ How do you make a model more robust to noisy data?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Use data augmentation** to expose the model to variations.  \n",
    "- **Apply regularization** (Dropout, L2).  \n",
    "- **Train with adversarial examples** to improve robustness.  \n",
    "\n",
    "‚úÖ **Code Example (Using Dropout for Regularization)**  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),  # Helps prevent overfitting to noise\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Your model overfits on training data but fails on new Etsy listings (cold start problem). What do you do?**  \n",
    "‚úÖ **Answer:**  \n",
    "- Use **zero-shot learning** to generalize to unseen categories.  \n",
    "- Use **transfer learning** from similar tasks.  \n",
    "- **Introduce metadata features** (e.g., category, seller history).  \n",
    "- **Start new listings with hybrid ranking** (heuristics + ML).  \n",
    "\n",
    "‚úÖ **Code Example (Transfer Learning to Handle Cold Start)**  \n",
    "```python\n",
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ Your model is sensitive to small input changes (adversarial attacks). How do you improve robustness?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Train on adversarial examples** to make the model resistant.(`adversarial attacks`: carefully crafted input perturbations that cause incorrect predictions) \n",
    "- **Use defensive distillation** to smooth model predictions.  \n",
    "- **Apply input normalization** to reduce sensitivity to noise.  \n",
    "\n",
    "‚úÖ **Code Example (Generating Adversarial Examples for Training)**  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def adversarial_pattern(model, image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(label, prediction)\n",
    "    gradient = tape.gradient(loss, image)\n",
    "    perturbation = tf.sign(gradient)\n",
    "    return perturbation\n",
    "\n",
    "# Use perturbation to train on adversarial examples\n",
    "adv_image = X_train[0] + 0.1 * adversarial_pattern(model, X_train[0], y_train[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ How do you make sure your model doesn‚Äôt make confident wrong predictions?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Calibrate the model** (e.g., Temperature Scaling).  \n",
    "- **Use probabilistic models** instead of deterministic ones.  \n",
    "- **Reject uncertain predictions** (set a confidence threshold).  \n",
    "\n",
    "‚úÖ **Code Example (Temperature Scaling for Calibration)**  \n",
    "```python\n",
    "def temperature_scale(logits, T=2.0):\n",
    "    return tf.nn.softmax(logits / T)\n",
    "\n",
    "logits = model.predict(X_test)\n",
    "calibrated_probs = temperature_scale(logits)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7Ô∏è‚É£ Your ranking model scores irrelevant ads highly. How do you debug feature importance?**  \n",
    "‚úÖ **Answer:**  \n",
    "- Use **SHAP (SHapley Additive Explanations)** to analyze feature importance.  \n",
    "- Check for **feature leakage** (data in training but not available at inference).  \n",
    "- Introduce **new ranking signals** to improve relevance.  \n",
    "\n",
    "‚úÖ **Code Example (Using SHAP to Analyze Feature Importance in a Ranking Model)**  \n",
    "```python\n",
    "import shap\n",
    "\n",
    "# Explain model predictions using SHAP\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot feature importance\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8Ô∏è‚É£ How do you detect and fix dataset shifts (distribution shifts)?**  \n",
    "‚úÖ **Answer:**  \n",
    "- Compare **training vs. inference data distributions**.  \n",
    "- Use **KL divergence** or **Jensen-Shannon distance** to detect shifts.  \n",
    "- Continuously **retrain the model on fresh data**.  \n",
    "\n",
    "**KL divergence is a fundamental concept in machine learning and statistics for measuring how different two probability distributions are**\n",
    "\n",
    "‚úÖ **Code Example (Detecting Dataset Shift with KL Divergence)**  \n",
    "```python\n",
    "import scipy.stats\n",
    "\n",
    "# Compute KL divergence between training and new data distributions\n",
    "kl_div = scipy.stats.entropy(np.histogram(X_train, bins=50)[0], np.histogram(X_test, bins=50)[0])\n",
    "print(\"KL Divergence:\", kl_div)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Multi-Task Learning for Ad Ranking**\n",
    "**Problem:**  \n",
    "Your ranking model must predict both **CTR** and **conversion probability**. How do you design a **multi-task learning (MTL) model**?  \n",
    "\n",
    "**Solution:**  \n",
    "- **Shared Encoder, Task-Specific Heads**:  \n",
    "  ```python\n",
    "  base = keras.Sequential([layers.Dense(128, activation='relu'), layers.Dense(64, activation='relu')])\n",
    "\n",
    "  ctr_head = layers.Dense(1, activation='sigmoid', name='ctr_output')(base.output)\n",
    "  conv_head = layers.Dense(1, activation='sigmoid', name='conv_output')(base.output)\n",
    "\n",
    "  model = keras.Model(inputs=base.input, outputs=[ctr_head, conv_head])\n",
    "  model.compile(loss={'ctr_output': 'binary_crossentropy', 'conv_output': 'binary_crossentropy'},\n",
    "                loss_weights={'ctr_output': 0.6, 'conv_output': 0.4}, optimizer='adam')\n",
    "  ```\n",
    "- **Loss Weights** balance importance of CTR vs. conversion.  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Key Takeaways**  \n",
    "‚úÖ **Handling Missing Data:** Imputation, flagging missing values.  \n",
    "‚úÖ **Class Imbalance:** Class weights, oversampling, synthetic data.  \n",
    "‚úÖ **Robustness to Noisy Data:** Dropout, adversarial training.  \n",
    "‚úÖ **Cold Start Problem:** Transfer learning, hybrid ranking.  \n",
    "‚úÖ **Adversarial Robustness:** Training on adversarial examples.  \n",
    "‚úÖ **Prediction Calibration:** Temperature scaling (Fixes Overconfidence: Modern deep learning models tend to assign high probability to incorrect predictions. Temperature scaling adjusts this), rejecting low-confidence predictions.  \n",
    "‚úÖ **Feature Importance Debugging:** SHAP for model explainability.  \n",
    "‚úÖ **Dataset Shift Detection:** KL divergence, retraining strategies.  \n",
    "\n",
    "Would you like **hands-on exercises** for these topics? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 10 easy to medium-level TensorFlow interview questions focused on **handling edge cases and model robustness**:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. How can you handle missing or corrupted data in a TensorFlow pipeline?** (BR)\n",
    "- **Answer:**  \n",
    "  - Use `tf.data.Dataset` methods like `.filter()` to remove corrupted data.\n",
    "  - Apply `.map()` with a function to fill missing values (e.g., using mean/median or a placeholder).\n",
    "  - Use `tf.where()` or `tf.fill()` to replace missing values during preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What techniques can you use to make a TensorFlow model more robust to outliers in the data?**\n",
    "- **Answer:**  \n",
    "  - Normalize or standardize input data to reduce the impact of outliers.\n",
    "  - Use robust loss functions like Huber loss instead of Mean Squared Error (MSE).\n",
    "  - Apply data augmentation to increase diversity in the training set.\n",
    "  - Clip gradients during training to prevent large updates from outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. How do you handle class imbalance in TensorFlow for classification tasks?**\n",
    "- **Answer:**  \n",
    "  - Use class weights in the loss function (`tf.keras.losses` supports `class_weight`).\n",
    "  - Oversample minority classes or undersample majority classes using `tf.data.Dataset`.\n",
    "  - Use data augmentation for minority classes.\n",
    "  - Evaluate metrics like F1-score or AUC instead of accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What is overfitting, and how can you prevent it in TensorFlow models?**\n",
    "- **Answer:**  \n",
    "  Overfitting occurs when a model performs well on training data but poorly on unseen data. Prevention techniques include:\n",
    "  - Adding dropout layers (`tf.keras.layers.Dropout`).\n",
    "  - Using L1/L2 regularization (`tf.keras.regularizers`).\n",
    "  - Early stopping (`tf.keras.callbacks.EarlyStopping`).\n",
    "  - Increasing training data or using data augmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. How can you ensure your TensorFlow model generalizes well to unseen data?**\n",
    "- **Answer:**  \n",
    "  - Split data into training, validation, and test sets.\n",
    "  - Use cross-validation during training.\n",
    "  - Monitor validation metrics (e.g., loss, accuracy) to detect overfitting.\n",
    "  - Apply regularization techniques like dropout and weight decay.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. How do you handle overfitting when working with small datasets in TensorFlow?**\n",
    "- **Answer:**  \n",
    "  - Use data augmentation to artificially increase dataset size.\n",
    "  - Transfer learning with pre-trained models (e.g., from TensorFlow Hub).\n",
    "  - Regularize the model with dropout and L2 regularization.\n",
    "  - Use smaller architectures to reduce model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. What is adversarial robustness, and how can you improve it in TensorFlow models?**\n",
    "- **Answer:**  \n",
    "  Adversarial robustness refers to a model's ability to resist adversarial attacks. Techniques to improve it include:\n",
    "  - Adversarial training: Train the model with adversarial examples.\n",
    "  - Gradient clipping to limit the impact of adversarial perturbations.\n",
    "  - Use defensive distillation or robust architectures.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. How can you debug a TensorFlow model that performs poorly on edge cases?**\n",
    "- **Answer:**  \n",
    "  - Analyze edge cases in the dataset and ensure they are represented in the training data.\n",
    "  - Use TensorBoard to visualize model predictions and identify failure modes.\n",
    "  - Apply data augmentation to include edge cases in training.\n",
    "  - Fine-tune the model or adjust loss functions to penalize errors on edge cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. How do you handle numerical instability in TensorFlow models?**\n",
    "- **Answer:**  \n",
    "  - Normalize input data to a reasonable range (e.g., [0, 1] or [-1, 1]).\n",
    "  - Use stable activation functions (e.g., ReLU instead of sigmoid/tanh).\n",
    "  - Clip gradients to prevent exploding gradients.\n",
    "  - Use mixed precision training to handle large values.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. What are some ways to test the robustness of a TensorFlow model?**\n",
    "- **Answer:**  \n",
    "  - Evaluate the model on a diverse test set, including edge cases.\n",
    "  - Perform stress testing with noisy or corrupted inputs.\n",
    "  - Use adversarial attacks to test robustness.\n",
    "  - Monitor performance across different subsets of data (e.g., by class or feature).\n",
    "\n",
    "---\n",
    "\n",
    "**Problem:**  \n",
    "You build a **product recommendation model** for Etsy users. A seller complains that their items never get recommended. What could be causing this, and how would you address it?  \n",
    "\n",
    "**Solution:**  \n",
    "- **Cold Start Problem**: New sellers don't have enough engagement data.  \n",
    "  - ‚úÖ Use **content-based features** (e.g., item descriptions, images) to recommend new products.  \n",
    "  - ‚úÖ Employ **multi-task learning** (predict CTR + item popularity).  \n",
    "\n",
    "- **Algorithm Bias**: Popular items get more exposure (rich-get-richer).  \n",
    "  - ‚úÖ Use **exploration techniques (e.g., Thompson Sampling, epsilon-greedy)**.  \n",
    "  - ‚úÖ Introduce **fairness constraints** to ensure new sellers get visibility.  \n",
    "\n",
    "---\n",
    "\n",
    "These questions cover key aspects of handling edge cases and ensuring model robustness in TensorFlow, from data preprocessing to advanced techniques like adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
